{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00a94b47-f47b-420f-ba7e-714ef219c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jobspy import scrape_jobs\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import csv, datetime, time, os\n",
    "import random, glob\n",
    "\n",
    "# Hybrid(office) proximity\n",
    "# --- Later ---\n",
    "# LLM ethos check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f773e6c-d9fc-42cc-b0ef-63b739e78435",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09283dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 20240220\n",
      "Keywords done: 0 : []\n",
      "Keywords left: 14 : ['python', 'database', 'nlp', 'natural language', 'signal processing', 'prompt engineer', 'openai', 'language model', 'generative model', 'genai', 'ki', 'künstliche intelligenz', 'ai', 'artificial intelligence']\n",
      "Keyword: database -> Starting search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucky/Documents/scripts/agent_jobspy/JobSpy/src/jobspy/__init__.py:155: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  jobs_df = pd.concat(jobs_dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of (unique) jobs found: 12\n",
      "Date: 20240220\n",
      "Keywords done: 1 : ['database']\n",
      "Keywords left: 13 : ['python', 'nlp', 'natural language', 'signal processing', 'prompt engineer', 'openai', 'language model', 'generative model', 'genai', 'ki', 'künstliche intelligenz', 'ai', 'artificial intelligence']\n",
      "Keyword: natural language -> Starting search...\n",
      "Error with keyword: natural language\n",
      "Could not find any results for the search\n",
      "Date: 20240220\n",
      "Keywords done: 2 : ['database', 'natural language']\n",
      "Keywords left: 12 : ['python', 'nlp', 'signal processing', 'prompt engineer', 'openai', 'language model', 'generative model', 'genai', 'ki', 'künstliche intelligenz', 'ai', 'artificial intelligence']\n",
      "Keyword: generative model -> Starting search...\n",
      "Error with keyword: generative model\n",
      "Could not find any results for the search\n",
      "Date: 20240220\n",
      "Keywords done: 3 : ['database', 'natural language', 'generative model']\n",
      "Keywords left: 11 : ['python', 'nlp', 'signal processing', 'prompt engineer', 'openai', 'language model', 'genai', 'ki', 'künstliche intelligenz', 'ai', 'artificial intelligence']\n",
      "Keyword: nlp -> Starting search...\n",
      "Error with keyword: nlp\n",
      "Could not find any results for the search\n",
      "Date: 20240220\n",
      "Keywords done: 4 : ['database', 'nlp', 'natural language', 'generative model']\n",
      "Keywords left: 10 : ['python', 'signal processing', 'prompt engineer', 'openai', 'language model', 'genai', 'ki', 'künstliche intelligenz', 'ai', 'artificial intelligence']\n",
      "Keyword: language model -> Starting search...\n",
      "Number of (unique) jobs found: 3\n",
      "Date: 20240220\n",
      "Keywords done: 5 : ['database', 'nlp', 'natural language', 'language model', 'generative model']\n",
      "Keywords left: 9 : ['python', 'signal processing', 'prompt engineer', 'openai', 'genai', 'ki', 'künstliche intelligenz', 'ai', 'artificial intelligence']\n",
      "Keyword: ai -> Starting search...\n",
      "Number of (unique) jobs found: 25\n",
      "Date: 20240220\n",
      "Keywords done: 6 : ['database', 'nlp', 'natural language', 'language model', 'generative model', 'ai']\n",
      "Keywords left: 8 : ['python', 'signal processing', 'prompt engineer', 'openai', 'genai', 'ki', 'künstliche intelligenz', 'artificial intelligence']\n",
      "Keyword: openai -> Starting search...\n",
      "Error with keyword: openai\n",
      "Could not find any results for the search\n",
      "Date: 20240220\n",
      "Keywords done: 7 : ['database', 'nlp', 'natural language', 'openai', 'language model', 'generative model', 'ai']\n",
      "Keywords left: 7 : ['python', 'signal processing', 'prompt engineer', 'genai', 'ki', 'künstliche intelligenz', 'artificial intelligence']\n",
      "Keyword: signal processing -> Starting search...\n",
      "Error with keyword: signal processing\n",
      "Could not find any results for the search\n",
      "Date: 20240220\n",
      "Keywords done: 8 : ['database', 'nlp', 'natural language', 'signal processing', 'openai', 'language model', 'generative model', 'ai']\n",
      "Keywords left: 6 : ['python', 'prompt engineer', 'genai', 'ki', 'künstliche intelligenz', 'artificial intelligence']\n",
      "Keyword: python -> Starting search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucky/Documents/scripts/agent_jobspy/JobSpy/src/jobspy/__init__.py:155: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  jobs_df = pd.concat(jobs_dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of (unique) jobs found: 27\n",
      "Date: 20240220\n",
      "Keywords done: 9 : ['python', 'database', 'nlp', 'natural language', 'signal processing', 'openai', 'language model', 'generative model', 'ai']\n",
      "Keywords left: 5 : ['prompt engineer', 'genai', 'ki', 'künstliche intelligenz', 'artificial intelligence']\n",
      "Keyword: genai -> Starting search...\n",
      "Error with keyword: genai\n",
      "Could not find any results for the search\n",
      "Date: 20240220\n",
      "Keywords done: 10 : ['python', 'database', 'nlp', 'natural language', 'signal processing', 'openai', 'language model', 'generative model', 'genai', 'ai']\n",
      "Keywords left: 4 : ['prompt engineer', 'ki', 'künstliche intelligenz', 'artificial intelligence']\n",
      "Keyword: künstliche intelligenz -> Starting search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucky/Documents/scripts/agent_jobspy/JobSpy/src/jobspy/__init__.py:155: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  jobs_df = pd.concat(jobs_dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of (unique) jobs found: 16\n",
      "Date: 20240220\n",
      "Keywords done: 11 : ['python', 'database', 'nlp', 'natural language', 'signal processing', 'openai', 'language model', 'generative model', 'genai', 'künstliche intelligenz', 'ai']\n",
      "Keywords left: 3 : ['prompt engineer', 'ki', 'artificial intelligence']\n",
      "Keyword: artificial intelligence -> Starting search...\n",
      "Number of (unique) jobs found: 2\n",
      "Date: 20240220\n",
      "Keywords done: 12 : ['python', 'database', 'nlp', 'natural language', 'signal processing', 'openai', 'language model', 'generative model', 'genai', 'künstliche intelligenz', 'ai', 'artificial intelligence']\n",
      "Keywords left: 2 : ['prompt engineer', 'ki']\n",
      "Keyword: prompt engineer -> Starting search...\n",
      "Error with keyword: prompt engineer\n",
      "Could not find any results for the search\n",
      "Date: 20240220\n",
      "Keywords done: 13 : ['python', 'database', 'nlp', 'natural language', 'signal processing', 'prompt engineer', 'openai', 'language model', 'generative model', 'genai', 'künstliche intelligenz', 'ai', 'artificial intelligence']\n",
      "Keywords left: 1 : ['ki']\n",
      "Keyword: ki -> Starting search...\n",
      "Number of (unique) jobs found: 33\n",
      "Date: 20240220\n",
      "Keywords done: 14 : ['python', 'database', 'nlp', 'natural language', 'signal processing', 'prompt engineer', 'openai', 'language model', 'generative model', 'genai', 'ki', 'künstliche intelligenz', 'ai', 'artificial intelligence']\n",
      "Keywords left: 0 : []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucky/Documents/scripts/agent_jobspy/JobSpy/src/jobspy/__init__.py:155: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  jobs_df = pd.concat(jobs_dfs, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "keywords = ['python', 'database', 'nlp','natural language',\n",
    "            'signal processing', 'prompt engineer', 'openai', 'language model', 'generative model',\n",
    "            'genai','ki', 'künstliche intelligenz', 'ai', 'artificial intelligence']\n",
    "\n",
    "def update_keywords_left(keywords: list, date: str) -> list:\n",
    "    \n",
    "    files = glob.glob(\"results/*.csv\")\n",
    "    files = [os.path.basename(file) for file in files]\n",
    "    keywords_done = [keyword for keyword in keywords if f\"{keyword}_{date}.csv\" in files]\n",
    "    keywords_left = [keyword for keyword in keywords if keyword not in keywords_done]\n",
    "\n",
    "    print(f\"Date: {date}\")\n",
    "    print(f\"Keywords done: {len(keywords_done)} : {keywords_done}\")\n",
    "    print(f\"Keywords left: {len(keywords_left)} : {keywords_left}\")\n",
    "    return keywords_left\n",
    "\n",
    "def get_jobs(keywords: list) -> None:\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    date = now.strftime(\"%Y%m%d\")\n",
    "\n",
    "    keywords_left = update_keywords_left(keywords, date)\n",
    "    \n",
    "    proxy_path = os.path.join(\"results\", \"proxy.txt\")\n",
    "    with open(proxy_path, \"r\") as f:\n",
    "        proxy = f.read().strip()\n",
    "    \n",
    "    empty_jobs = pd.DataFrame(columns=[ \"job_url\",\n",
    "    \"site\", \"title\", \"company\", \"company_url\", \"location\", \"job_type\",\n",
    "    \"date_posted\", \"interval\", \"min_amount\", \"max_amount\", \"currency\",\n",
    "    \"is_remote\", \"num_urgent_words\", \"benefits\", \"emails\", \"description\"])\n",
    "    \n",
    "    while keywords_left:\n",
    "    \n",
    "        keyword = random.choice(keywords_left)\n",
    "        kw_path = os.path.join(\"results\", f\"{keyword}_{date}.csv\")\n",
    "\n",
    "        print(f\"Keyword: {keyword} -> Starting search...\")\n",
    "\n",
    "        try:\n",
    "            jobs = scrape_jobs(\n",
    "                site_name=[\"indeed\"],\n",
    "\n",
    "                search_term=keyword,\n",
    "                proxy=proxy,\n",
    "\n",
    "                hours_old=72,\n",
    "                is_remote=True,\n",
    "                results_wanted=500,\n",
    "                country_indeed='germany'  # only needed for indeed / glassdoor\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with keyword: {keyword}\")\n",
    "            print(e)\n",
    "\n",
    "            if \"Bad proxy\" in str(e):\n",
    "                # stop the program if the proxy is bad\n",
    "                print(\"Bad proxy, stopping the program\")\n",
    "                break\n",
    "\n",
    "            if \"Could not find any results for the search\" in str(e):\n",
    "                empty_jobs.to_csv(kw_path, quoting=csv.QUOTE_NONNUMERIC, escapechar=\"\\\\\", index=False)\n",
    "                keywords_left = update_keywords_left(keywords, date)\n",
    "            continue\n",
    "\n",
    "        jobs=jobs.drop_duplicates(subset=[\"job_url\"], keep=\"first\")\n",
    "        # listing with the same title from the same company are considered duplicates, because\n",
    "        # they are usually the same job offer, just posted multiple times for different locations\n",
    "        jobs=jobs.drop_duplicates(subset=[\"title\", \"company\"], keep=\"first\")\n",
    "\n",
    "        print(f\"Number of (unique) jobs found: {len(jobs)}\")\n",
    "\n",
    "        \n",
    "        jobs.to_csv(kw_path, quoting=csv.QUOTE_NONNUMERIC, escapechar=\"\\\\\", index=False)\n",
    "        keywords_left = update_keywords_left(keywords, date)\n",
    "\n",
    "get_jobs(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dedc33b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates: {'20240217', '20240220', '20240218'}\n",
      " --- 15 Keywords for 20240217 : ['signal processing_20240217.csv', 'sigma_20240217.csv', 'python_20240217.csv', 'prompt engineer_20240217.csv', 'openai_20240217.csv', 'nlp_20240217.csv', 'natural language_20240217.csv', 'language model_20240217.csv', 'künstliche intelligenz_20240217.csv', 'ki_20240217.csv', 'generative model_20240217.csv', 'genai_20240217.csv', 'database_20240217.csv', 'artificial intelligence_20240217.csv', 'ai_20240217.csv']\n",
      "+----------------------------------------------------+--------+\n",
      "| File                                               | Length |\n",
      "+----------------------------------------------------+-------+\n",
      "| signal processing_20240217.csv                     |     1 |\n",
      "| sigma_20240217.csv                                 |   143 |\n",
      "| python_20240217.csv                                |    61 |\n",
      "| prompt engineer_20240217.csv                       |     1 |\n",
      "| openai_20240217.csv                                |     0 |\n",
      "| nlp_20240217.csv                                   |     3 |\n",
      "| natural language_20240217.csv                      |     5 |\n",
      "| language model_20240217.csv                        |    18 |\n",
      "| künstliche intelligenz_20240217.csv                |    20 |\n",
      "| ki_20240217.csv                                    |    29 |\n",
      "| generative model_20240217.csv                      |     1 |\n",
      "| genai_20240217.csv                                 |     0 |\n",
      "| database_20240217.csv                              |    15 |\n",
      "| artificial intelligence_20240217.csv               |     5 |\n",
      "| ai_20240217.csv                                    |    38 |\n",
      "+----------------------------------------------------+--------+\n",
      " --- 15 Keywords for 20240220 : ['signal processing_20240220.csv', 'sigma_20240220.csv', 'python_20240220.csv', 'prompt engineer_20240220.csv', 'openai_20240220.csv', 'nlp_20240220.csv', 'natural language_20240220.csv', 'language model_20240220.csv', 'künstliche intelligenz_20240220.csv', 'ki_20240220.csv', 'generative model_20240220.csv', 'genai_20240220.csv', 'database_20240220.csv', 'artificial intelligence_20240220.csv', 'ai_20240220.csv']\n",
      "+----------------------------------------------------+--------+\n",
      "| File                                               | Length |\n",
      "+----------------------------------------------------+-------+\n",
      "| signal processing_20240220.csv                     |     0 |\n",
      "| sigma_20240220.csv                                 |    96 |\n",
      "| python_20240220.csv                                |    27 |\n",
      "| prompt engineer_20240220.csv                       |     0 |\n",
      "| openai_20240220.csv                                |     0 |\n",
      "| nlp_20240220.csv                                   |     0 |\n",
      "| natural language_20240220.csv                      |     0 |\n",
      "| language model_20240220.csv                        |     3 |\n",
      "| künstliche intelligenz_20240220.csv                |    16 |\n",
      "| ki_20240220.csv                                    |    33 |\n",
      "| generative model_20240220.csv                      |     0 |\n",
      "| genai_20240220.csv                                 |     0 |\n",
      "| database_20240220.csv                              |    12 |\n",
      "| artificial intelligence_20240220.csv               |     2 |\n",
      "| ai_20240220.csv                                    |    25 |\n",
      "+----------------------------------------------------+--------+\n",
      " --- 10 Keywords for 20240218 : ['signal processing_20240218.csv', 'sigma_20240218.csv', 'python_20240218.csv', 'prompt engineer_20240218.csv', 'openai_20240218.csv', 'natural language_20240218.csv', 'künstliche intelligenz_20240218.csv', 'database_20240218.csv', 'artificial intelligence_20240218.csv', 'ai_20240218.csv']\n",
      "+----------------------------------------------------+--------+\n",
      "| File                                               | Length |\n",
      "+----------------------------------------------------+-------+\n",
      "| signal processing_20240218.csv                     |     1 |\n",
      "| sigma_20240218.csv                                 |    95 |\n",
      "| python_20240218.csv                                |    42 |\n",
      "| prompt engineer_20240218.csv                       |     0 |\n",
      "| openai_20240218.csv                                |     0 |\n",
      "| natural language_20240218.csv                      |     3 |\n",
      "| künstliche intelligenz_20240218.csv                |    18 |\n",
      "| database_20240218.csv                              |    13 |\n",
      "| artificial intelligence_20240218.csv               |     3 |\n",
      "| ai_20240218.csv                                    |    31 |\n",
      "+----------------------------------------------------+--------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Assuming file_names and dates are defined\\n\\nfile_names = [\"example_20200101.csv\", \"sample_20200102.csv\"]  # Example file names\\ndates = [\"20200101\", \"20200102\"]  # Example dates\\n\\nfor input_date in dates:\\n    keywords_date = [file for file in file_names if input_date in file]\\n    lengths = []\\n\\n    for file in keywords_date:\\n        df = pd.read_csv(os.path.join(\"results\", file))\\n        lengths.append(len(df))\\n    \\n    # Printing the ASCII table\\n    print(f\" --- {len(keywords_date)} Keywords for {input_date} : {keywords_date}\")\\n    print(\"+------------+--------+\")\\n    print(\"| File       | Length |\")\\n    print(\"+------------+--------+\")\\n    for file, length in zip(keywords_date, lengths):\\n        print(f\"| {file:} | {length:6} |\")\\n    print(\"+------------+--------+\")\\n\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check dates and show results - meta table\n",
    "\n",
    "file_paths = glob.glob(\"results/*.csv\")\n",
    "\n",
    "file_names = [os.path.basename(file) for file in file_paths]\n",
    "file_names = sorted(file_names, reverse=True)\n",
    "\n",
    "dates = set([file.split(\"_\")[1].split(\".\")[0] for file in file_names])\n",
    "print(f\"Dates: {dates}\")\n",
    "\n",
    "for input_date in dates:\n",
    "\n",
    "    keywords_date = [file for file in file_names if input_date in file]\n",
    "    length = len(keywords_date)\n",
    "    lengths = []\n",
    "\n",
    "    print(f\" --- {length} Keywords for {input_date} : {keywords_date}\")\n",
    "\n",
    "    # print the length of the dataframes vertically in table format\n",
    "\n",
    "    for file in keywords_date:\n",
    "        df = pd.read_csv(os.path.join(\"results\", file))\n",
    "        lengths.append(len(df))\n",
    "    \n",
    "    # Printing the ASCII table\n",
    "    print(\"+\"+\"-\"*52+\"+\"+\"--------+\")\n",
    "    print(\"| File\"+\" \"*47+\"| Length |\")\n",
    "    print(\"+\"+\"-\"*52+\"+\"+\"-------+\")\n",
    "    for file, length in zip(keywords_date, lengths):\n",
    "        print(f\"| {file:50} | {length:5} |\")\n",
    "    print(\"+\"+\"-\"*52+\"+\"+\"--------+\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Assuming file_names and dates are defined\n",
    "\n",
    "file_names = [\"example_20200101.csv\", \"sample_20200102.csv\"]  # Example file names\n",
    "dates = [\"20200101\", \"20200102\"]  # Example dates\n",
    "\n",
    "for input_date in dates:\n",
    "    keywords_date = [file for file in file_names if input_date in file]\n",
    "    lengths = []\n",
    "\n",
    "    for file in keywords_date:\n",
    "        df = pd.read_csv(os.path.join(\"results\", file))\n",
    "        lengths.append(len(df))\n",
    "    \n",
    "    # Printing the ASCII table\n",
    "    print(f\" --- {len(keywords_date)} Keywords for {input_date} : {keywords_date}\")\n",
    "    print(\"+------------+--------+\")\n",
    "    print(\"| File       | Length |\")\n",
    "    print(\"+------------+--------+\")\n",
    "    for file, length in zip(keywords_date, lengths):\n",
    "        print(f\"| {file:} | {length:6} |\")\n",
    "    print(\"+------------+--------+\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12ffa47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 96 entries, 0 to 117\n",
      "Data columns (total 16 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   job_url           96 non-null     object \n",
      " 1   site              96 non-null     object \n",
      " 2   title             96 non-null     object \n",
      " 3   company           96 non-null     object \n",
      " 4   company_url       92 non-null     object \n",
      " 5   location          96 non-null     object \n",
      " 6   job_type          62 non-null     object \n",
      " 7   date_posted       96 non-null     object \n",
      " 8   interval          13 non-null     object \n",
      " 9   min_amount        13 non-null     float64\n",
      " 10  max_amount        13 non-null     float64\n",
      " 11  currency          13 non-null     object \n",
      " 12  is_remote         96 non-null     object \n",
      " 13  num_urgent_words  61 non-null     float64\n",
      " 14  benefits          0 non-null      float64\n",
      " 15  emails            13 non-null     object \n",
      "dtypes: float64(4), object(12)\n",
      "memory usage: 12.8+ KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26313/2427281813.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sigma_date = pd.concat([sigma_date, df], ignore_index=True)\n",
      "/tmp/ipykernel_26313/2427281813.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sigma_date = pd.concat([sigma_date, df], ignore_index=True)\n",
      "/tmp/ipykernel_26313/2427281813.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sigma_date = pd.concat([sigma_date, df], ignore_index=True)\n",
      "/tmp/ipykernel_26313/2427281813.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sigma_date = pd.concat([sigma_date, df], ignore_index=True)\n",
      "/tmp/ipykernel_26313/2427281813.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sigma_date = pd.concat([sigma_date, df], ignore_index=True)\n",
      "/tmp/ipykernel_26313/2427281813.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sigma_date = pd.concat([sigma_date, df], ignore_index=True)\n",
      "/tmp/ipykernel_26313/2427281813.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sigma_date = pd.concat([sigma_date, df], ignore_index=True)\n",
      "/tmp/ipykernel_26313/2427281813.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sigma_date = pd.concat([sigma_date, df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Create SIGMA for date - concatenate all csv files into one by date    \n",
    "\n",
    "now = datetime.datetime.now()   \n",
    "date = now.strftime(\"%Y%m%d\")\n",
    "\n",
    "\n",
    "search_str = f\"results/*{date}.csv\"\n",
    "file_names = glob.glob(\"results/*.csv\")\n",
    "\n",
    "sigma_date = pd.DataFrame(columns=[ \"job_url\",\n",
    "    \"site\", \"title\", \"company\", \"company_url\", \"location\", \"job_type\",\n",
    "    \"date_posted\", \"interval\", \"min_amount\", \"max_amount\", \"currency\",\n",
    "    \"is_remote\", \"num_urgent_words\", \"benefits\", \"emails\", \"description\"])\n",
    "\n",
    "for file in file_names:\n",
    "    if date in file:\n",
    "        df = pd.read_csv(file)\n",
    "        sigma_date = pd.concat([sigma_date, df], ignore_index=True)\n",
    "\n",
    "# remove duplicates\n",
    "sigma_date = sigma_date.drop_duplicates(subset=[\"job_url\"], keep=\"first\")\n",
    "sigma_date = sigma_date.drop_duplicates(subset=[\"title\", \"company\"], keep=\"first\")\n",
    "\n",
    "\n",
    "# drop description column\n",
    "\n",
    "sigma_date = sigma_date.drop(columns=[\"description\"])\n",
    "\n",
    "sigma_date.to_csv(f\"results/sigma_{date}.csv\", quoting=csv.QUOTE_NONNUMERIC, escapechar=\"\\\\\", index=False)\n",
    "print(sigma_date.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83ad06f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26313/4098255643.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sigma_prev = pd.concat([sigma_prev, df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest sigma entries: 96\n",
      "New jobs found: 86\n",
      "Not opening job urls\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# determine the latest sigma file, remove it from the list, concatenate the rest\n",
    "sigma_files = glob.glob(\"results/sigma_*.csv\")\n",
    "sigma_dates = [file.split(\"_\")[1].split(\".\")[0] for file in sigma_files]\n",
    "sigma_dates = [int(date) for date in sigma_dates]\n",
    "\n",
    "latest_sigma_date = max(sigma_dates)\n",
    "latest_sigma_path = f\"results/sigma_{latest_sigma_date}.csv\"\n",
    "sigma_files = [file for file in sigma_files if file != latest_sigma_path]\n",
    "\n",
    "sigma_latest = pd.read_csv(latest_sigma_path)\n",
    "sigma_prev = pd.DataFrame(columns=[ \"job_url\",\n",
    "    \"site\", \"title\", \"company\", \"company_url\", \"location\", \"job_type\",\n",
    "    \"date_posted\", \"interval\", \"min_amount\", \"max_amount\", \"currency\",\n",
    "    \"is_remote\", \"num_urgent_words\", \"benefits\", \"emails\", \"description\"])\n",
    "\n",
    "for file in sigma_files:\n",
    "    df = pd.read_csv(file)\n",
    "    sigma_prev = pd.concat([sigma_prev, df], ignore_index=True)\n",
    "\n",
    "# delete all entries from sigma latest that are also in sigma prev\n",
    "sigma = sigma_latest[~sigma_latest[\"job_url\"].isin(sigma_prev[\"job_url\"])]\n",
    "\n",
    "print(f\"Latest sigma entries: {len(sigma_latest)}\")\n",
    "print(f\"New jobs found: {len(sigma)}\")\n",
    "\n",
    "input = input(\"Open all job urls? (y/n): \")\n",
    "if input == \"y\":\n",
    "\n",
    "    for url in sigma[\"job_url\"]:\n",
    "        os.system(f\"open {url}\")\n",
    "else:\n",
    "    print(\"Not opening job urls\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e37a521-caef-441c-8fc2-2eb5b2e7da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use if hyperlinks=True\n",
    "html = jobs.to_html(escape=False)\n",
    "# change max-width: 200px to show more or less of the content\n",
    "truncate_width = f'<style>.dataframe td {{ max-width: 200px; overflow: hidden; text-overflow: ellipsis; white-space: nowrap; }}</style>{html}'\n",
    "display(HTML(truncate_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd992a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dough, Sauce ,Toppings - 3 Pizzaz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
