{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00a94b47-f47b-420f-ba7e-714ef219c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jobspy import scrape_jobs\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import csv, datetime, time, os, json\n",
    "import random, glob\n",
    "from pprint import pprint\n",
    "\n",
    "# Hybrid(office) proximity\n",
    "# --- Later ---\n",
    "# LLM ethos check\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f773e6c-d9fc-42cc-b0ef-63b739e78435",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2aa3b976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User tdawg already exists\n",
      "['users/tdawg/tdawg_config.json', 'users/lucky/lucky_config.json']\n",
      "{'user': 'tdawg', 'email': 'test@test.de'}\n",
      "{'user': 'lucky', 'email': 's4mipojo@uni-trier.de'}\n"
     ]
    }
   ],
   "source": [
    "# Import user configs\n",
    "\n",
    "def insert_user(user, email):\n",
    "    \n",
    "    if not os.path.exists(f'users/{user}'):\n",
    "\n",
    "        os.mkdir(f'users/{user}')\n",
    "        user_dict = {'user': user, 'email': email}\n",
    "        config_path = os.path.join(f'users/{user}', f'{user}_config.json')\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(user_dict, f, indent=4)\n",
    "\n",
    "    else :\n",
    "        print(f'User {user} already exists')\n",
    "\n",
    "\n",
    "#insert_user('tdawg', 'test@test.de')\n",
    "\n",
    "\n",
    "basic_PJ = {\n",
    "    \"platform\":\"indeed\", \n",
    "    \"location\": \"England-remote\", \n",
    "    \"kewwords\": [\"python\", \"data\"],\n",
    "            \n",
    "    \"days_left\": 7,\n",
    "    \"interval\": \"daily\"\n",
    "    }\n",
    "\n",
    "\n",
    "user_config_paths = glob.glob('users/**/*.json')\n",
    "print(user_config_paths)\n",
    "\n",
    "user_configs = []\n",
    "for user_config_path in user_config_paths:\n",
    "    with open(user_config_path) as f:\n",
    "        tmp_dict = json.load(f)\n",
    "        user_configs.append(tmp_dict)\n",
    "\n",
    "for user in user_configs:\n",
    "   print(user)\n",
    "\n",
    "\n",
    "\n",
    "def order_PJ(user, P):\n",
    "    pass\n",
    "\n",
    "def show_stats():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09283dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "keywords = ['python', 'database', 'nlp','natural language',\n",
    "            'signal processing', 'prompt engineer', 'openai', 'language model', 'generative model',\n",
    "            'genai','ki', 'künstliche intelligenz', 'ai', 'artificial intelligence']\n",
    "\n",
    "def update_keywords_left(keywords: list, date: str) -> list:\n",
    "    \n",
    "    files = glob.glob(\"results/*.csv\")\n",
    "    files = [os.path.basename(file) for file in files]\n",
    "    keywords_done = [keyword for keyword in keywords if f\"{keyword}_{date}.csv\" in files]\n",
    "    keywords_left = [keyword for keyword in keywords if keyword not in keywords_done]\n",
    "\n",
    "    print(f\"Date: {date}\")\n",
    "    print(f\"Keywords done: {len(keywords_done)} : {keywords_done}\")\n",
    "    print(f\"Keywords left: {len(keywords_left)} : {keywords_left}\")\n",
    "    return keywords_left\n",
    "\n",
    "def get_jobs(keywords: list) -> None:\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    date = now.strftime(\"%Y%m%d\")\n",
    "\n",
    "    keywords_left = update_keywords_left(keywords, date)\n",
    "    \n",
    "    proxy_path = os.path.join(\"results\", \"proxy.txt\")\n",
    "    with open(proxy_path, \"r\") as f:\n",
    "        proxy = f.read().strip()\n",
    "    \n",
    "    empty_jobs = pd.DataFrame(columns=[ \"job_url\",\n",
    "    \"site\", \"title\", \"company\", \"company_url\", \"location\", \"job_type\",\n",
    "    \"date_posted\", \"interval\", \"min_amount\", \"max_amount\", \"currency\",\n",
    "    \"is_remote\", \"num_urgent_words\", \"benefits\", \"emails\", \"description\"])\n",
    "    \n",
    "    while keywords_left:\n",
    "    \n",
    "        keyword = random.choice(keywords_left)\n",
    "        kw_path = os.path.join(\"results\", f\"{keyword}_{date}.csv\")\n",
    "\n",
    "        print(f\"Keyword: {keyword} -> Starting search...\")\n",
    "\n",
    "        try:\n",
    "            jobs = scrape_jobs(\n",
    "                site_name=[\"indeed\"],\n",
    "\n",
    "                search_term=keyword,\n",
    "                proxy=proxy,\n",
    "\n",
    "                hours_old=72,\n",
    "                is_remote=True,\n",
    "                results_wanted=500,\n",
    "                country_indeed='germany'  # only needed for indeed / glassdoor\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with keyword: {keyword}\")\n",
    "            print(e)\n",
    "\n",
    "            if \"Bad proxy\" in str(e):\n",
    "                # stop the program if the proxy is bad\n",
    "                print(\"Bad proxy, stopping the program\")\n",
    "                break\n",
    "\n",
    "            if \"Could not find any results for the search\" in str(e):\n",
    "                empty_jobs.to_csv(kw_path, quoting=csv.QUOTE_NONNUMERIC, escapechar=\"\\\\\", index=False)\n",
    "                keywords_left = update_keywords_left(keywords, date)\n",
    "            continue\n",
    "\n",
    "        jobs=jobs.drop_duplicates(subset=[\"job_url\"], keep=\"first\")\n",
    "        # listing with the same title from the same company are considered duplicates, because\n",
    "        # they are usually the same job offer, just posted multiple times for different locations\n",
    "        jobs=jobs.drop_duplicates(subset=[\"title\", \"company\"], keep=\"first\")\n",
    "\n",
    "        print(f\"Number of (unique) jobs found: {len(jobs)}\")\n",
    "\n",
    "        \n",
    "        jobs.to_csv(kw_path, quoting=csv.QUOTE_NONNUMERIC, escapechar=\"\\\\\", index=False)\n",
    "        keywords_left = update_keywords_left(keywords, date)\n",
    "\n",
    "get_jobs(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dedc33b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates: {'20240218', '20240217', '20240220'}\n",
      " --- 10 Keywords for 20240218 : ['signal processing_20240218.csv', 'sigma_20240218.csv', 'python_20240218.csv', 'prompt engineer_20240218.csv', 'openai_20240218.csv', 'natural language_20240218.csv', 'künstliche intelligenz_20240218.csv', 'database_20240218.csv', 'artificial intelligence_20240218.csv', 'ai_20240218.csv']\n",
      "+----------------------------------------------------+--------+\n",
      "| File                                               | Length |\n",
      "+----------------------------------------------------+--------+\n",
      "| signal processing_20240218.csv                     |      1 |\n",
      "| sigma_20240218.csv                                 |     95 |\n",
      "| python_20240218.csv                                |     42 |\n",
      "| prompt engineer_20240218.csv                       |      0 |\n",
      "| openai_20240218.csv                                |      0 |\n",
      "| natural language_20240218.csv                      |      3 |\n",
      "| künstliche intelligenz_20240218.csv                |     18 |\n",
      "| database_20240218.csv                              |     13 |\n",
      "| artificial intelligence_20240218.csv               |      3 |\n",
      "| ai_20240218.csv                                    |     31 |\n",
      "+----------------------------------------------------+--------+\n",
      " --- 15 Keywords for 20240217 : ['signal processing_20240217.csv', 'sigma_20240217.csv', 'python_20240217.csv', 'prompt engineer_20240217.csv', 'openai_20240217.csv', 'nlp_20240217.csv', 'natural language_20240217.csv', 'language model_20240217.csv', 'künstliche intelligenz_20240217.csv', 'ki_20240217.csv', 'generative model_20240217.csv', 'genai_20240217.csv', 'database_20240217.csv', 'artificial intelligence_20240217.csv', 'ai_20240217.csv']\n",
      "+----------------------------------------------------+--------+\n",
      "| File                                               | Length |\n",
      "+----------------------------------------------------+--------+\n",
      "| signal processing_20240217.csv                     |      1 |\n",
      "| sigma_20240217.csv                                 |    143 |\n",
      "| python_20240217.csv                                |     61 |\n",
      "| prompt engineer_20240217.csv                       |      1 |\n",
      "| openai_20240217.csv                                |      0 |\n",
      "| nlp_20240217.csv                                   |      3 |\n",
      "| natural language_20240217.csv                      |      5 |\n",
      "| language model_20240217.csv                        |     18 |\n",
      "| künstliche intelligenz_20240217.csv                |     20 |\n",
      "| ki_20240217.csv                                    |     29 |\n",
      "| generative model_20240217.csv                      |      1 |\n",
      "| genai_20240217.csv                                 |      0 |\n",
      "| database_20240217.csv                              |     15 |\n",
      "| artificial intelligence_20240217.csv               |      5 |\n",
      "| ai_20240217.csv                                    |     38 |\n",
      "+----------------------------------------------------+--------+\n",
      " --- 15 Keywords for 20240220 : ['signal processing_20240220.csv', 'sigma_20240220.csv', 'python_20240220.csv', 'prompt engineer_20240220.csv', 'openai_20240220.csv', 'nlp_20240220.csv', 'natural language_20240220.csv', 'language model_20240220.csv', 'künstliche intelligenz_20240220.csv', 'ki_20240220.csv', 'generative model_20240220.csv', 'genai_20240220.csv', 'database_20240220.csv', 'artificial intelligence_20240220.csv', 'ai_20240220.csv']\n",
      "+----------------------------------------------------+--------+\n",
      "| File                                               | Length |\n",
      "+----------------------------------------------------+--------+\n",
      "| signal processing_20240220.csv                     |      0 |\n",
      "| sigma_20240220.csv                                 |     96 |\n",
      "| python_20240220.csv                                |     27 |\n",
      "| prompt engineer_20240220.csv                       |      0 |\n",
      "| openai_20240220.csv                                |      0 |\n",
      "| nlp_20240220.csv                                   |      0 |\n",
      "| natural language_20240220.csv                      |      0 |\n",
      "| language model_20240220.csv                        |      3 |\n",
      "| künstliche intelligenz_20240220.csv                |     16 |\n",
      "| ki_20240220.csv                                    |     33 |\n",
      "| generative model_20240220.csv                      |      0 |\n",
      "| genai_20240220.csv                                 |      0 |\n",
      "| database_20240220.csv                              |     12 |\n",
      "| artificial intelligence_20240220.csv               |      2 |\n",
      "| ai_20240220.csv                                    |     25 |\n",
      "+----------------------------------------------------+--------+\n"
     ]
    }
   ],
   "source": [
    "# check dates and show results - meta table\n",
    "\n",
    "file_paths = glob.glob(\"results/*.csv\")\n",
    "\n",
    "file_names = [os.path.basename(file) for file in file_paths]\n",
    "file_names = sorted(file_names, reverse=True)\n",
    "\n",
    "dates = set([file.split(\"_\")[1].split(\".\")[0] for file in file_names])\n",
    "print(f\"Dates: {dates}\")\n",
    "\n",
    "for input_date in dates:\n",
    "\n",
    "    keywords_date = [file for file in file_names if input_date in file]\n",
    "    length = len(keywords_date)\n",
    "    lengths = []\n",
    "\n",
    "    print(f\" --- {length} Keywords for {input_date} : {keywords_date}\")\n",
    "\n",
    "    # print the length of the dataframes vertically in table format\n",
    "\n",
    "    for file in keywords_date:\n",
    "        df = pd.read_csv(os.path.join(\"results\", file))\n",
    "        lengths.append(len(df))\n",
    "    \n",
    "    # Printing the ASCII table\n",
    "    print(\"+\"+\"-\"*52+\"+\"+\"--------+\")\n",
    "    print(\"| File\"+\" \"*47+\"| Length |\")\n",
    "    print(\"+\"+\"-\"*52+\"+\"+\"--------+\")\n",
    "    for file, length in zip(keywords_date, lengths):\n",
    "        print(f\"| {file:50} | {length:6} |\")\n",
    "    print(\"+\"+\"-\"*52+\"+\"+\"--------+\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ffa47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SIGMA for date - concatenate all csv files into one by date    \n",
    "\n",
    "now = datetime.datetime.now()   \n",
    "date = now.strftime(\"%Y%m%d\")\n",
    "\n",
    "\n",
    "search_str = f\"results/*{date}.csv\"\n",
    "file_names = glob.glob(\"results/*.csv\")\n",
    "\n",
    "sigma_date = pd.DataFrame(columns=[ \"job_url\",\n",
    "    \"site\", \"title\", \"company\", \"company_url\", \"location\", \"job_type\",\n",
    "    \"date_posted\", \"interval\", \"min_amount\", \"max_amount\", \"currency\",\n",
    "    \"is_remote\", \"num_urgent_words\", \"benefits\", \"emails\", \"description\"])\n",
    "\n",
    "for file in file_names:\n",
    "    if date in file:\n",
    "        df = pd.read_csv(file)\n",
    "        sigma_date = pd.concat([sigma_date, df], ignore_index=True)\n",
    "\n",
    "# remove duplicates\n",
    "sigma_date = sigma_date.drop_duplicates(subset=[\"job_url\"], keep=\"first\")\n",
    "sigma_date = sigma_date.drop_duplicates(subset=[\"title\", \"company\"], keep=\"first\")\n",
    "\n",
    "\n",
    "# drop description column\n",
    "\n",
    "sigma_date = sigma_date.drop(columns=[\"description\"])\n",
    "\n",
    "sigma_date.to_csv(f\"results/sigma_{date}.csv\", quoting=csv.QUOTE_NONNUMERIC, escapechar=\"\\\\\", index=False)\n",
    "print(sigma_date.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ad06f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# determine the latest sigma file, remove it from the list, concatenate the rest\n",
    "sigma_files = glob.glob(\"results/sigma_*.csv\")\n",
    "sigma_dates = [file.split(\"_\")[1].split(\".\")[0] for file in sigma_files]\n",
    "sigma_dates = [int(date) for date in sigma_dates]\n",
    "\n",
    "latest_sigma_date = max(sigma_dates)\n",
    "latest_sigma_path = f\"results/sigma_{latest_sigma_date}.csv\"\n",
    "sigma_files = [file for file in sigma_files if file != latest_sigma_path]\n",
    "\n",
    "sigma_latest = pd.read_csv(latest_sigma_path)\n",
    "sigma_prev = pd.DataFrame(columns=[ \"job_url\",\n",
    "    \"site\", \"title\", \"company\", \"company_url\", \"location\", \"job_type\",\n",
    "    \"date_posted\", \"interval\", \"min_amount\", \"max_amount\", \"currency\",\n",
    "    \"is_remote\", \"num_urgent_words\", \"benefits\", \"emails\", \"description\"])\n",
    "\n",
    "for file in sigma_files:\n",
    "    df = pd.read_csv(file)\n",
    "    sigma_prev = pd.concat([sigma_prev, df], ignore_index=True)\n",
    "\n",
    "# delete all entries from sigma latest that are also in sigma prev\n",
    "sigma = sigma_latest[~sigma_latest[\"job_url\"].isin(sigma_prev[\"job_url\"])]\n",
    "\n",
    "print(f\"Latest sigma entries: {len(sigma_latest)}\")\n",
    "print(f\"New jobs found: {len(sigma)}\")\n",
    "\n",
    "cli_in = input(\"Open all job urls? (y/n): \")\n",
    "if cli_in == \"y\":\n",
    "\n",
    "    for url in sigma[\"job_url\"]:\n",
    "        os.system(f\"open {url}\")\n",
    "else:\n",
    "    print(\"Not opening job urls\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e37a521-caef-441c-8fc2-2eb5b2e7da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use if hyperlinks=True\n",
    "html = jobs.to_html(escape=False)\n",
    "# change max-width: 200px to show more or less of the content\n",
    "truncate_width = f'<style>.dataframe td {{ max-width: 200px; overflow: hidden; text-overflow: ellipsis; white-space: nowrap; }}</style>{html}'\n",
    "display(HTML(truncate_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd992a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dough, Sauce ,Toppings - 3 Pizzaz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
