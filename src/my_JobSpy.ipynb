{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00a94b47-f47b-420f-ba7e-714ef219c006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n #  \\n    usig date folders with kw as csv files to denote states (ACID-drops) and progress\\n    search config kw might change over time which is why we will store the kw in csv files\\n\\n    - users\\n        - name and email\\n        - jobs_configs\\n        - result_jobs - dates[keywords], sigma_raw (csv), sigma_quo(function) \\n        - result_stats - eg. ger_nlp_remote, lux_bwl, sigma\\n        \\n        \\n-> Stats - Per job_config & Per User\\n         - \\n\\n        kw1     kw2     ...    kwN     Total-Date\\n\\ndate1\\n\\ndate2\\n\\n...\\n\\ndateN\\n\\nTotal-KW\\n    \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jobspy import scrape_jobs\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import csv, datetime, time, os, json\n",
    "import random, glob\n",
    "from pprint import pprint\n",
    "\n",
    "# Hybrid(office) proximity\n",
    "# --- Later ---\n",
    "# LLM ethos check\n",
    "# logging, shadowbranching when time is right\n",
    "\n",
    "\"\"\"\n",
    " #  \n",
    "    usig date folders with kw as csv files to denote states (ACID-drops) and progress\n",
    "    search config kw might change over time which is why we will store the kw in csv files\n",
    "\n",
    "    - users\n",
    "        - name and email\n",
    "        - jobs_configs\n",
    "        - result_jobs - dates[keywords], sigma_raw (csv), sigma_quo(function) \n",
    "        - result_stats - eg. ger_nlp_remote, lux_bwl, sigma\n",
    "        \n",
    "        \n",
    "-> Stats - Per job_config & Per User\n",
    "         - \n",
    "\n",
    "        kw1     kw2     ...    kwN     Total-Date\n",
    "\n",
    "date1\n",
    "\n",
    "date2\n",
    "\n",
    "...\n",
    "\n",
    "dateN\n",
    "\n",
    "Total-KW\n",
    "    \n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f773e6c-d9fc-42cc-b0ef-63b739e78435",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aa3b976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['users/tdawg/tdawg_config.json', 'users/lucky/lucky_config.json']\n",
      "{'email': 'test@test.de', 'user': 'tdawg'}\n",
      "{'email': 's4mipojo@uni-trier.de',\n",
      " 'jobs': [{'country_indeed': 'luxembourg',\n",
      "           'created': '2016-01-01T00:00:00Z',\n",
      "           'hours_old': 72,\n",
      "           'id': 1,\n",
      "           'is_remote': False,\n",
      "           'name': 'lux_py',\n",
      "           'proxy': 'proxy',\n",
      "           'results_wanted': 500,\n",
      "           'run_count': 0,\n",
      "           'run_left': 0,\n",
      "           'search_terms': ['', 'python', 'database'],\n",
      "           'site_name': ['indeed'],\n",
      "           'updated': '2016-01-01T00:00:00Z'},\n",
      "          {'country_indeed': 'germany',\n",
      "           'created': '2016-01-01T00:00:00Z',\n",
      "           'hours_old': 72,\n",
      "           'id': 2,\n",
      "           'is_remote': True,\n",
      "           'name': 'ger_py',\n",
      "           'proxy': 'proxy',\n",
      "           'results_wanted': 500,\n",
      "           'search_terms': ['', 'python', 'database'],\n",
      "           'site_name': ['indeed'],\n",
      "           'status': 'running',\n",
      "           'updated': '2016-01-01T00:00:00Z'}],\n",
      " 'user': 'lucky'}\n"
     ]
    }
   ],
   "source": [
    "# Import user configs\n",
    "\n",
    "def insert_user(user, email):\n",
    "    \n",
    "    if not os.path.exists(f'users/{user}'):\n",
    "\n",
    "        os.mkdir(f'users/{user}')\n",
    "        user_dict = {'user': user, 'email': email}\n",
    "        config_path = os.path.join(f'users/{user}', f'{user}_config.json')\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(user_dict, f, indent=4)\n",
    "\n",
    "    else :\n",
    "        print(f'User {user} already exists')\n",
    "\n",
    "\n",
    "#insert_user('tdawg', 'test@test.de')\n",
    "\n",
    "\n",
    "basic_PJ = {\n",
    "    \"platform\":\"indeed\", \n",
    "    \"location\": \"England-remote\", \n",
    "    \"kewwords\": [\"python\", \"data\"],\n",
    "            \n",
    "    \"days_left\": 7,\n",
    "    \"interval\": \"daily\"\n",
    "    }\n",
    "\n",
    "\n",
    "user_config_paths = glob.glob('users/**/*.json')\n",
    "print(user_config_paths)\n",
    "\n",
    "user_configs = []\n",
    "for user_config_path in user_config_paths:\n",
    "    with open(user_config_path) as f:\n",
    "        tmp_dict = json.load(f)\n",
    "        user_configs.append(tmp_dict)\n",
    "\n",
    "for user in user_configs:\n",
    "   pprint(user)\n",
    "\n",
    "\n",
    "\n",
    "def order_PJ(user, P):\n",
    "    pass\n",
    "\n",
    "def show_stats():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09283dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "keywords = ['python', 'database', 'nlp','natural language',\n",
    "            'signal processing', 'prompt engineer', 'openai', 'language model', 'generative model',\n",
    "            'genai','ki', 'künstliche intelligenz', 'ai', 'artificial intelligence']\n",
    "\n",
    "def update_keywords_left(keywords: list, date: str) -> list:\n",
    "    \n",
    "    files = glob.glob(\"results/*.csv\")\n",
    "    files = [os.path.basename(file) for file in files]\n",
    "    keywords_done = [keyword for keyword in keywords if f\"{keyword}_{date}.csv\" in files]\n",
    "    keywords_left = [keyword for keyword in keywords if keyword not in keywords_done]\n",
    "\n",
    "    print(f\"--- Date: {date} ---\")\n",
    "    print(f\"Keywords done: {len(keywords_done)} : {keywords_done}\")\n",
    "    print(f\"Keywords left: {len(keywords_left)} : {keywords_left}\")\n",
    "    return keywords_left\n",
    "\n",
    "def get_jobs(keywords: list) -> None:\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    date = now.strftime(\"%Y%m%d\")\n",
    "\n",
    "    keywords_left = update_keywords_left(keywords, date)\n",
    "    \n",
    "    proxy_path = os.path.join(\"results\", \"proxy.txt\")\n",
    "    with open(proxy_path, \"r\") as f:\n",
    "        proxy = f.read().strip()\n",
    "    \n",
    "    empty_jobs = pd.DataFrame(columns=[ \"job_url\",\n",
    "    \"site\", \"title\", \"company\", \"company_url\", \"location\", \"job_type\",\n",
    "    \"date_posted\", \"interval\", \"min_amount\", \"max_amount\", \"currency\",\n",
    "    \"is_remote\", \"num_urgent_words\", \"benefits\", \"emails\", \"description\"])\n",
    "    jobs = empty_jobs\n",
    "\n",
    "    while keywords_left:\n",
    "    \n",
    "        keyword = random.choice(keywords_left)\n",
    "        kw_path = os.path.join(\"results\", f\"{keyword}_{date}.csv\")\n",
    "\n",
    "        print(f\"Keyword: *** {keyword} *** -> Starting search...\")\n",
    "\n",
    "        try:\n",
    "            jobs = scrape_jobs(\n",
    "                site_name=[\"indeed\"],\n",
    "\n",
    "                search_term=keyword,\n",
    "                proxy=proxy,\n",
    "\n",
    "                hours_old=72,\n",
    "                #is_remote=True, # Lux does not seem to work with remote\n",
    "                results_wanted=500,\n",
    "                country_indeed='luxembourg'#'germany'  # only needed for indeed / glassdoor\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with keyword: {keyword}\")\n",
    "            print(e)\n",
    "\n",
    "            if \"Bad proxy\" in str(e):\n",
    "                # if the proxy is not working, the program will never advance, so we stop it.\n",
    "                print(\"Bad proxy, stopping the program\")\n",
    "\n",
    "                \" TODO: Outbound email to notify the admin, maybe change proxy\"\n",
    "                break\n",
    "\n",
    "            # There is also : \"HTTPSConnectionPool(host='apis.indeed.com', port=443): Max retries \n",
    "            # exceeded with url: /graphql (Caused by ProxyError('Unable to connect to proxy', \n",
    "            # RemoteDisconnected('Remote end closed connection without response')))\" Error but\n",
    "            # but the file being written is not necessarily empty.\n",
    "\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        print(f\"Number of jobs found: {len(jobs)}\")\n",
    "        jobs = jobs.drop_duplicates(subset=[\"job_url\"], keep=\"first\")\n",
    "        # listing with the same title from the same company are considered duplicates, because\n",
    "        # they are usually the same job offer, just posted multiple times for different locations\n",
    "        jobs = jobs.drop_duplicates(subset=[\"title\", \"company\"], keep=\"first\")\n",
    "        print(f\"Number of (unique) jobs found: {len(jobs)}\")\n",
    "\n",
    "        if jobs.empty:\n",
    "            print(f\"No jobs found for keyword: {keyword}. Writing empty file...\")\n",
    "            empty_jobs.to_csv(kw_path, quoting=csv.QUOTE_NONNUMERIC, escapechar=\"\\\\\", index=False)\n",
    "            \n",
    "        else:\n",
    "            print(f\"Writing jobs to file: {kw_path}\")\n",
    "            jobs.to_csv(kw_path, quoting=csv.QUOTE_NONNUMERIC, escapechar=\"\\\\\", index=False)\n",
    "            jobs = empty_jobs\n",
    "        \n",
    "        keywords_left = update_keywords_left(keywords, date)\n",
    "\n",
    "get_jobs(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dedc33b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates: {'20240227'}\n",
      " --- 15 Keywords for 20240227 : ['signal processing_20240227.csv', 'sigma_20240227.csv', 'python_20240227.csv', 'prompt engineer_20240227.csv', 'openai_20240227.csv', 'nlp_20240227.csv', 'natural language_20240227.csv', 'language model_20240227.csv', 'künstliche intelligenz_20240227.csv', 'ki_20240227.csv', 'generative model_20240227.csv', 'genai_20240227.csv', 'database_20240227.csv', 'artificial intelligence_20240227.csv', 'ai_20240227.csv']\n",
      "+----------------------------------------------------+--------+\n",
      "| File                                               | Length |\n",
      "+----------------------------------------------------+--------+\n",
      "| signal processing_20240227.csv                     |      0 |\n",
      "| sigma_20240227.csv                                 |     31 |\n",
      "| python_20240227.csv                                |     11 |\n",
      "| prompt engineer_20240227.csv                       |      0 |\n",
      "| openai_20240227.csv                                |      0 |\n",
      "| nlp_20240227.csv                                   |      0 |\n",
      "| natural language_20240227.csv                      |      1 |\n",
      "| language model_20240227.csv                        |     12 |\n",
      "| künstliche intelligenz_20240227.csv                |      0 |\n",
      "| ki_20240227.csv                                    |      0 |\n",
      "| generative model_20240227.csv                      |      0 |\n",
      "| genai_20240227.csv                                 |      0 |\n",
      "| database_20240227.csv                              |      7 |\n",
      "| artificial intelligence_20240227.csv               |      4 |\n",
      "| ai_20240227.csv                                    |      0 |\n",
      "+----------------------------------------------------+--------+\n"
     ]
    }
   ],
   "source": [
    "# check dates and show results - meta table\n",
    "\n",
    "file_paths = glob.glob(\"results/*.csv\")\n",
    "\n",
    "file_names = [os.path.basename(file) for file in file_paths]\n",
    "file_names = sorted(file_names, reverse=True)\n",
    "\n",
    "dates = set([file.split(\"_\")[1].split(\".\")[0] for file in file_names])\n",
    "print(f\"Dates: {dates}\")\n",
    "\n",
    "for input_date in dates:\n",
    "\n",
    "    keywords_date = [file for file in file_names if input_date in file]\n",
    "    length = len(keywords_date)\n",
    "    lengths = []\n",
    "\n",
    "    print(f\" --- {length} Keywords for {input_date} : {keywords_date}\")\n",
    "\n",
    "    # print the length of the dataframes vertically in table format\n",
    "\n",
    "    for file in keywords_date:\n",
    "        df = pd.read_csv(os.path.join(\"results\", file))\n",
    "        lengths.append(len(df))\n",
    "    \n",
    "    # Printing the ASCII table\n",
    "    print(\"+\"+\"-\"*52+\"+\"+\"--------+\")\n",
    "    print(\"| File\"+\" \"*47+\"| Length |\")\n",
    "    print(\"+\"+\"-\"*52+\"+\"+\"--------+\")\n",
    "    for file, length in zip(keywords_date, lengths):\n",
    "        print(f\"| {file:50} | {length:6} |\")\n",
    "    print(\"+\"+\"-\"*52+\"+\"+\"--------+\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12ffa47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 31 entries, 0 to 33\n",
      "Data columns (total 16 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   job_url           31 non-null     object \n",
      " 1   site              31 non-null     object \n",
      " 2   title             31 non-null     object \n",
      " 3   company           31 non-null     object \n",
      " 4   company_url       31 non-null     object \n",
      " 5   location          31 non-null     object \n",
      " 6   job_type          15 non-null     object \n",
      " 7   date_posted       31 non-null     object \n",
      " 8   interval          2 non-null      object \n",
      " 9   min_amount        2 non-null      float64\n",
      " 10  max_amount        2 non-null      float64\n",
      " 11  currency          2 non-null      object \n",
      " 12  is_remote         31 non-null     object \n",
      " 13  num_urgent_words  20 non-null     float64\n",
      " 14  benefits          0 non-null      object \n",
      " 15  emails            1 non-null      object \n",
      "dtypes: float64(3), object(13)\n",
      "memory usage: 4.1+ KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2489232/2427281813.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sigma_date = pd.concat([sigma_date, df], ignore_index=True)\n",
      "/tmp/ipykernel_2489232/2427281813.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sigma_date = pd.concat([sigma_date, df], ignore_index=True)\n",
      "/tmp/ipykernel_2489232/2427281813.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sigma_date = pd.concat([sigma_date, df], ignore_index=True)\n",
      "/tmp/ipykernel_2489232/2427281813.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sigma_date = pd.concat([sigma_date, df], ignore_index=True)\n",
      "/tmp/ipykernel_2489232/2427281813.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sigma_date = pd.concat([sigma_date, df], ignore_index=True)\n",
      "/tmp/ipykernel_2489232/2427281813.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sigma_date = pd.concat([sigma_date, df], ignore_index=True)\n",
      "/tmp/ipykernel_2489232/2427281813.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sigma_date = pd.concat([sigma_date, df], ignore_index=True)\n",
      "/tmp/ipykernel_2489232/2427281813.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sigma_date = pd.concat([sigma_date, df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Create SIGMA for date - concatenate all csv files into one by date    \n",
    "\n",
    "now = datetime.datetime.now()   \n",
    "date = now.strftime(\"%Y%m%d\")\n",
    "\n",
    "\n",
    "search_str = f\"results/*{date}.csv\"\n",
    "file_names = glob.glob(\"results/*.csv\")\n",
    "\n",
    "sigma_date = pd.DataFrame(columns=[ \"job_url\",\n",
    "    \"site\", \"title\", \"company\", \"company_url\", \"location\", \"job_type\",\n",
    "    \"date_posted\", \"interval\", \"min_amount\", \"max_amount\", \"currency\",\n",
    "    \"is_remote\", \"num_urgent_words\", \"benefits\", \"emails\", \"description\"])\n",
    "\n",
    "for file in file_names:\n",
    "    if date in file:\n",
    "        df = pd.read_csv(file)\n",
    "        sigma_date = pd.concat([sigma_date, df], ignore_index=True)\n",
    "\n",
    "# remove duplicates\n",
    "sigma_date = sigma_date.drop_duplicates(subset=[\"job_url\"], keep=\"first\")\n",
    "sigma_date = sigma_date.drop_duplicates(subset=[\"title\", \"company\"], keep=\"first\")\n",
    "\n",
    "\n",
    "# drop description column\n",
    "\n",
    "sigma_date = sigma_date.drop(columns=[\"description\"])\n",
    "\n",
    "sigma_date.to_csv(f\"results/sigma_{date}.csv\", quoting=csv.QUOTE_NONNUMERIC, escapechar=\"\\\\\", index=False)\n",
    "print(sigma_date.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83ad06f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest sigma entries: 31\n",
      "New jobs found: 31\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n",
      "Opening in existing browser session.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# determine the latest sigma file, remove it from the list, concatenate the rest\n",
    "sigma_files = glob.glob(\"results/sigma_*.csv\")\n",
    "sigma_dates = [file.split(\"_\")[1].split(\".\")[0] for file in sigma_files]\n",
    "sigma_dates = [int(date) for date in sigma_dates]\n",
    "\n",
    "latest_sigma_date = max(sigma_dates)\n",
    "latest_sigma_path = f\"results/sigma_{latest_sigma_date}.csv\"\n",
    "sigma_files = [file for file in sigma_files if file != latest_sigma_path]\n",
    "\n",
    "sigma_latest = pd.read_csv(latest_sigma_path)\n",
    "sigma_prev = pd.DataFrame(columns=[ \"job_url\",\n",
    "    \"site\", \"title\", \"company\", \"company_url\", \"location\", \"job_type\",\n",
    "    \"date_posted\", \"interval\", \"min_amount\", \"max_amount\", \"currency\",\n",
    "    \"is_remote\", \"num_urgent_words\", \"benefits\", \"emails\", \"description\"])\n",
    "\n",
    "for file in sigma_files:\n",
    "    df = pd.read_csv(file)\n",
    "    sigma_prev = pd.concat([sigma_prev, df], ignore_index=True)\n",
    "\n",
    "# delete all entries from sigma latest that are also in sigma prev\n",
    "sigma = sigma_latest[~sigma_latest[\"job_url\"].isin(sigma_prev[\"job_url\"])]\n",
    "\n",
    "print(f\"Latest sigma entries: {len(sigma_latest)}\")\n",
    "print(f\"New jobs found: {len(sigma)}\")\n",
    "\n",
    "cli_in = input(\"Open all job urls? (y/n): \")\n",
    "if cli_in == \"y\":\n",
    "\n",
    "    for url in sigma[\"job_url\"]:\n",
    "        os.system(f\"open {url}\")\n",
    "else:\n",
    "    print(\"Not opening job urls\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e37a521-caef-441c-8fc2-2eb5b2e7da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use if hyperlinks=True\n",
    "html = jobs.to_html(escape=False)\n",
    "# change max-width: 200px to show more or less of the content\n",
    "truncate_width = f'<style>.dataframe td {{ max-width: 200px; overflow: hidden; text-overflow: ellipsis; white-space: nowrap; }}</style>{html}'\n",
    "display(HTML(truncate_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd992a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dough, Sauce ,Toppings - 3 Pizzaz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
