{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, glob, os\n",
    "import random, datetime\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from jobspy import scrape_jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "ids are unique, global static\n",
    "\n",
    "Agent Fisher\n",
    "    - run tru all users, their settings, and their keywords\n",
    "    - save the results in their corresponding folders\n",
    "\n",
    "Agent Kowalski\n",
    "    - ELT the data - remove duplicates, clean, etc\n",
    "    - save the results in their corresponding folders\n",
    "    - add new users and jobs. check if both are valid\n",
    "    - Generate Reports and Stats\n",
    "\n",
    "Agent Krieger\n",
    "    - run tru all jobs and apply to them\n",
    "\n",
    "\n",
    "    LOGGING\n",
    "\n",
    "    - folder structure does a lot for logging\n",
    "    - log all errors and warnings - and the files that triggered them\n",
    "    - saving files (advace state) is very permisive as of now.\n",
    "    - Empty files are not saved (they probably should be though)\n",
    "\n",
    "    -----------\n",
    "\n",
    "    - Log all errors and warnings\n",
    "    - Log all successful runs\n",
    "    - Log all failed runs\n",
    "\n",
    "     [RUN-LOGS-(package)-E-Mail]\n",
    "\n",
    "     encypted zip - stats.csv, sigma_quo.txt, \n",
    "\n",
    "     - polymorphic/other dp, encap, graph inherit\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---> Config file paths found : ['users/tdawg/tdawg_config.json', 'users/lucky/lucky_config.json']\n",
      "Running user: lucky\n",
      "Created folder: users/lucky/20240306\n",
      "Created folder: users/lucky/20240306/lux_py\n",
      "Running search setting: lux_py for user: lucky\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'keywords'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 228\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    227\u001b[0m agent_fish \u001b[38;5;241m=\u001b[39m Agent_Fisher()\n\u001b[0;32m--> 228\u001b[0m \u001b[43magent_fish\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_user\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_fish\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_configs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_logs\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[25], line 206\u001b[0m, in \u001b[0;36mAgent_Fisher.run_user\u001b[0;34m(self, user_config)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated folder: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch_setting_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning search setting: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch_setting[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for user: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00musername\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 206\u001b[0m     successful_run\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_search_setting\u001b[49m\u001b[43m(\u001b[49m\u001b[43musername\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_setting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdate_run\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(successful_run):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00musername\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - All search settings ran successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 112\u001b[0m, in \u001b[0;36mAgent_Fisher.run_search_setting\u001b[0;34m(self, username, search_setting, date)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_search_setting\u001b[39m(\u001b[38;5;28mself\u001b[39m, username: \u001b[38;5;28mstr\u001b[39m, search_setting: \u001b[38;5;28mdict\u001b[39m, date: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    107\u001b[0m     empty_jobs \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob_url\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msite\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompany\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompany_url\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob_type\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate_posted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterval\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_amount\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_amount\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrency\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_remote\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_urgent_words\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenefits\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memails\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 112\u001b[0m     keywords \u001b[38;5;241m=\u001b[39m \u001b[43msearch_setting\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkeywords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    113\u001b[0m     jobs \u001b[38;5;241m=\u001b[39m empty_jobs\n\u001b[1;32m    115\u001b[0m     ss_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musers\u001b[39m\u001b[38;5;124m\"\u001b[39m,username, date, search_setting[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'keywords'"
     ]
    }
   ],
   "source": [
    "class Agent_Perry():\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    in-file redudancy - duplicates, cross time\n",
    "    \n",
    "\n",
    "    # listing with the same title from the same company are considered duplicates, because\n",
    "        # they are usually the same job offer, just posted multiple times for different locations\n",
    "        jobs = jobs.drop_duplicates(subset=[\"title\", \"company\"], keep=\"first\")\n",
    "        print(f\"Number of (unique) jobs found: {len(jobs)}\")\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def filter_jobs(jobs: pd.DataFrame, search_setting: dict) -> pd.DataFrame:\n",
    "            pass\n",
    "    pass\n",
    "\n",
    "    pass\n",
    "\n",
    "class Agent_Krieger():\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "class Agent_Fisher():\n",
    "    def __init__(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Agent Fisher is the muscle of the operation. It is responsible for running the scrapes.\n",
    "\n",
    "        get_user_configs - returns a list of dictionaries, each dictionary is a user config.\n",
    "        get_proxy - returns a string of the proxy to be passed to the scraper as a string.\n",
    "        get_date - returns a string of the date in the format YYYYMMDD.\n",
    "        Date, user and search_settings(stored in user_configs - eg. IT jobs in london, \n",
    "        remote teaching jobs in Germany). \n",
    "        \"\"\"\n",
    "        \n",
    "        self.date_run = self.get_date()\n",
    "        self.proxy = self.get_proxy()\n",
    "        self.user_configs = self.get_user_configs()\n",
    "\n",
    "    # --- INIT Functions --- #\n",
    "\n",
    "    @staticmethod\n",
    "    def get_date() -> str:\n",
    "        now = datetime.datetime.now()\n",
    "        date = now.strftime(\"%Y%m%d\")\n",
    "        \n",
    "        return date\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_user_configs() -> list[dict]: \n",
    "\n",
    "        user_config_paths = glob.glob('users/**/*.json')\n",
    "        print(f\" ---> Config file paths found : {user_config_paths}\")\n",
    "        user_configs = []\n",
    "\n",
    "        for user_config_path in user_config_paths:\n",
    "            with open(user_config_path) as f:\n",
    "                tmp_dict = json.load(f)\n",
    "                user_configs.append(tmp_dict)\n",
    "        \n",
    "        return user_configs\n",
    "\n",
    "    @staticmethod\n",
    "    def get_proxy() -> str:\n",
    "        \n",
    "        proxy_path = os.path.join(\"results\", \"proxy.txt\")\n",
    "        with open(proxy_path, \"r\") as f:\n",
    "            proxy = f.read().strip()\n",
    "\n",
    "        return proxy\n",
    "\n",
    "\n",
    "    # --- SEARCH --- #\n",
    "\n",
    "    def update_keywords_left(self, kewords_for_this_search: list, ss_path: str) -> list[str]:\n",
    "\n",
    "        \"\"\"\n",
    "        This function checks which keywords have already been run and determines the state of the run.\n",
    "        If a keyword has already been run (clean, no exception thrown) it is removed from the list (state).\n",
    "        Exceptions are there to handle the case where the run was not completed, incomplete or the proxy\n",
    "        was blocked.\n",
    "\n",
    "        ss_path: str - user/date/search_setting - path to the search setting folder\n",
    "\n",
    "        Note - the order of the keywods run is picked randomly. To spice things up a bit ...\n",
    "        \"\"\"\n",
    "        \n",
    "        files = glob.glob(f\"{ss_path}/*.csv\") ### os path join\n",
    "        files = [os.path.basename(file) for file in files]\n",
    "\n",
    "        keywords_done = [keyword for keyword in kewords_for_this_search if f\"{keyword}.csv\" in files]\n",
    "        keywords_left = [keyword for keyword in kewords_for_this_search if keyword not in keywords_done]\n",
    "\n",
    "        print(f\"--- Path: {ss_path}/*.csv ---\")\n",
    "        print(f\"Keywords done: {len(keywords_done)} : {keywords_done}\")\n",
    "        print(f\"Keywords left: {len(keywords_left)} : {keywords_left}\")\n",
    "        return keywords_left\n",
    "\n",
    "\n",
    "\n",
    "    def run_search_setting(self, username: str, search_setting: dict, date: str) -> bool:\n",
    "            \n",
    "        empty_jobs = pd.DataFrame(columns=[ \"job_url\",\n",
    "        \"site\", \"title\", \"company\", \"company_url\", \"location\", \"job_type\",\n",
    "        \"date_posted\", \"interval\", \"min_amount\", \"max_amount\", \"currency\",\n",
    "        \"is_remote\", \"num_urgent_words\", \"benefits\", \"emails\", \"description\"])\n",
    "\n",
    "        keywords = search_setting['keywords']\n",
    "        jobs = empty_jobs\n",
    "\n",
    "        ss_path = os.path.join(\"users\",username, date, search_setting['name'])\n",
    "        keywords_left = self.update_keywords_left(keywords, ss_path)\n",
    "\n",
    "        while keywords_left:\n",
    "\n",
    "            keyword = random.choice(keywords_left)\n",
    "            kw_path = os.path.join(ss_path, f\"{keyword}.csv\")\n",
    "\n",
    "            print(f\"Keyword: *** {keyword} *** -> Starting search...\")\n",
    "\n",
    "            try:\n",
    "                jobs = scrape_jobs(\n",
    "                    site_name=search_setting['site_name'],\n",
    "\n",
    "                    search_term=keyword,\n",
    "                    proxy=self.proxy,\n",
    "\n",
    "                    hours_old=search_setting['hours_old'],\n",
    "                    is_remote=search_setting['is_remote'],\n",
    "                    results_wanted=search_setting['results_wanted'],\n",
    "                    country_indeed=search_setting['country_indeed']  # only needed for indeed / glassdoor\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error with keyword: {keyword}\")\n",
    "                print(e)\n",
    "\n",
    "                if \"Bad proxy\" in str(e):\n",
    "                    # if the proxy is not working, the program will never advance, so we stop it.\n",
    "                    print(\"Bad proxy, stopping the program\")\n",
    "\n",
    "                    \" TODO: Outbound email to notify the admin, maybe change proxy\"\n",
    "                    return False\n",
    "                    \n",
    "                # There is also : \"HTTPSConnectionPool(host='apis.indeed.com', port=443): Max retries \n",
    "                # exceeded with url: /graphql (Caused by ProxyError('Unable to connect to proxy', \n",
    "                # RemoteDisconnected('Remote end closed connection without response')))\" Error but\n",
    "                # but the file being written is not necessarily empty.\n",
    "\n",
    "                continue\n",
    "            \n",
    "            print(f\"Number of jobs found: {len(jobs)}\")\n",
    "            jobs = jobs.drop_duplicates(subset=[\"job_url\"], keep=\"first\")\n",
    "            \n",
    "            if jobs.empty:\n",
    "                print(f\"No jobs found for keyword: {keyword}. Writing empty file...\")\n",
    "                empty_jobs.to_csv(kw_path, quoting=csv.QUOTE_NONNUMERIC, escapechar=\"\\\\\", index=False)\n",
    "                \n",
    "            else:\n",
    "                print(f\"Writing jobs to file: {kw_path}\")\n",
    "                jobs.to_csv(kw_path, quoting=csv.QUOTE_NONNUMERIC, escapechar=\"\\\\\", index=False)\n",
    "                jobs = empty_jobs\n",
    "\n",
    "            keywords_left = self.update_keywords_left(keywords, ss_path)\n",
    "        \n",
    "        return True\n",
    "\n",
    "    # --- Overarching --- #\n",
    "\n",
    "    def run_user(self, user_config: dict) -> None:\n",
    "        \n",
    "        username = user_config['user']\n",
    "        print(f\"Running user: {username}\")\n",
    "        \n",
    "        successful_run = []\n",
    "\n",
    "        try:\n",
    "            search_settings = user_config['search_settings']\n",
    "        except KeyError:\n",
    "            print(f\"User: {username} - No search settings found - Key Error\")\n",
    "            return\n",
    "\n",
    "        if not search_settings:\n",
    "            print(f\"User: {username} - No search settings found - Empty List\")\n",
    "            return\n",
    "\n",
    "        for search_setting in user_config['search_settings']:\n",
    "\n",
    "            # try creqting the folder\n",
    "            date_path = os.path.join(\"users\", username, self.date_run)\n",
    "            search_setting_path = os.path.join(date_path, search_setting['name'])\n",
    "\n",
    "            #if folders already exist, we skip the creation. Take this into account for Perry\n",
    "            if not os.path.exists(date_path):\n",
    "                os.makedirs(date_path)\n",
    "                print(f\"Created folder: {date_path}\")\n",
    "            if not os.path.exists(search_setting_path):\n",
    "                os.makedirs(search_setting_path)\n",
    "                print(f\"Created folder: {search_setting_path}\")\n",
    "\n",
    "            print(f\"Running search setting: {search_setting['name']} for user: {username}\")\n",
    "            successful_run.append(self.run_search_setting(username, search_setting, self.date_run))\n",
    "\n",
    "        \n",
    "        \n",
    "        if all(successful_run):\n",
    "            print(f\"User: {username} - All search settings ran successfully\")\n",
    "        else:\n",
    "            print(f\"User: {username} - Some search settings failed. Run is incomplete\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def run_all_users(self):\n",
    "\n",
    "        #date = self.date_run\n",
    "        #user_configs = self.user_configs\n",
    "\n",
    "\n",
    "        #print(f\" ---> Date : {date}\")\n",
    "        pass\n",
    "\n",
    "agent_fish = Agent_Fisher()\n",
    "agent_fish.run_user(agent_fish.user_configs[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def send_logs() -> None:\n",
    "    pass\n",
    "\n",
    "def send_email() -> None:\n",
    "    pass\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-jobspy-gxDDdTXR-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
