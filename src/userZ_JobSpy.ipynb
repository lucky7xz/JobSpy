{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, glob, os\n",
    "import random, datetime\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from jobspy import scrape_jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "ids are unique, global static\n",
    "\n",
    "Agent Fisher\n",
    "    - run tru all users, their settings, and their keywords\n",
    "    - save the results in their corresponding folders\n",
    "\n",
    "Agent Perry\n",
    "`   \n",
    "    - add new users and jobs. check if both are valid\n",
    "\n",
    "    - ELT the data - remove duplicates, clean, etc\n",
    "    - save the (agg)results in their corresponding folders\n",
    "\n",
    "    - Generate Reports and Stats\n",
    "    - sent emails, text messages?\n",
    "\n",
    "Agent Krieger\n",
    "    - run tru all jobs and apply to them\n",
    "\n",
    "\n",
    "    LOGGING\n",
    "\n",
    "    - folder structure does a lot for logging\n",
    "    - log all errors and warnings - and the files that triggered them\n",
    "    - saving files (advace state) is ~ permisive as of now.\n",
    "\n",
    "    -----------\n",
    "\n",
    "    - Log all errors and warnings\n",
    "    - Log all successful runs\n",
    "    - Log all failed runs\n",
    "\n",
    "     [RUN-LOGS-(package)-E-Mail]\n",
    "\n",
    "     encypted zip - stats.csv, sigma_quo.txt, \n",
    "\n",
    "     - polymorphic/other dp, encap, graph inherit\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Agent_Krieger():\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " / ***Agent Perry is online *** /\n",
      "Instantiating Perry...\n",
      " ---> Config file paths found : ['users/tdawg/tdawg_config.json', 'users/lucky/lucky_config.json']\n",
      " ---> Getting Perry's To-Do List...\n",
      "User: tdawg - No search settings found - Key Error\n",
      "\n",
      " --- Destination folders for lucky: ---\n",
      "users/lucky/20240306\n",
      "    - ger_py --- OK\n",
      "    - lux_py --- OK\n",
      "{'lucky': {'20240306': ['users/lucky/20240306/ger_py', 'users/lucky/20240306/lux_py']}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Agent_Perry():\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    Perry is the Brain of the operation. He is responsible for the ELT of the data, and the \n",
    "    generation of reports and stats.\n",
    "    \n",
    "    for each user,\n",
    "        for each date\n",
    "            one dataframe with the columns found in the kw csv (standard) as well as date and \n",
    "            config folder columns to identify the source of the data\n",
    "\n",
    "\n",
    "    # listing with the same title from the same company are considered duplicates, because\n",
    "        # they are usually the same job offer, just posted multiple times for different locations\n",
    "        jobs = jobs.drop_duplicates(subset=[\"title\", \"company\"], keep=\"first\")\n",
    "        print(f\"Number of (unique) jobs found: {len(jobs)}\")\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "          \n",
    "          print(\" / ***Agent Perry is online *** /\")\n",
    "          print(\"Instantiating Perry...\")\n",
    "          self.user_configs = self.get_user_configs()\n",
    "          self.perrys_todo = self.get_perrys_todo()\n",
    "    \n",
    "    # using the config files you can determine the users\n",
    "    # then determine the dates for each user\n",
    "    # then determine the search configs for each user\n",
    "\n",
    "    @staticmethod # redundant, given it is a static method chekc later\n",
    "    def get_user_configs() -> list[dict]: \n",
    "\n",
    "        user_config_paths = glob.glob('users/**/*.json')\n",
    "        print(f\" ---> Config file paths found : {user_config_paths}\")\n",
    "        user_configs = []\n",
    "\n",
    "        for user_config_path in user_config_paths:\n",
    "            with open(user_config_path) as f:\n",
    "                tmp_dict = json.load(f)\n",
    "                user_configs.append(tmp_dict)\n",
    "        \n",
    "        return user_configs\n",
    "\n",
    "\n",
    "    def get_perrys_todo(self) -> dict:\n",
    "\n",
    "        \"\"\"\n",
    "        This function helps with the ELT of the data. It returns a dictionary, which contains is \n",
    "        constructed to easily iterate over the data and apply the ELT process to it.\n",
    "\n",
    "        \"\"\"\n",
    "        print(\" ---> Getting Perry's To-Do List...\")\n",
    "        perrys_todo = {}\n",
    "\n",
    "        for user_config in self.user_configs:\n",
    "            username = user_config['user']\n",
    "\n",
    "            try:\n",
    "                #each search config folder we find has to be in this list, otherwise it can't be right.\n",
    "                search_settings_user = [search_settings['name'] for search_settings in user_config['search_settings']]\n",
    "                perrys_todo[username] = {}\n",
    "\n",
    "            except KeyError:\n",
    "                print(f\"User: {username} - No search settings found - Key Error\")\n",
    "                continue\n",
    "\n",
    "            if not search_settings_user:\n",
    "                print(f\"User: {username} - No search settings found - Empty List\")\n",
    "                continue\n",
    "\n",
    "            \n",
    "            dates = glob.glob(f'users/{username}/*')\n",
    "            dates = [os.path.basename(date) for date in dates if date.endswith('.json') == False]  \n",
    "\n",
    "            for date in dates:\n",
    "                search_setting_folder_paths = glob.glob(f'users/{username}/{date}/*')\n",
    "                perrys_todo[username][date] = search_setting_folder_paths\n",
    "\n",
    "\n",
    "            # for each date in each user --->\n",
    "            #   [username][date] = destination folder\n",
    "            #   [username][date][search_setting_folderpath] = source folder(s)\n",
    "            #   ... -> {'lucky': {'20240306': ['users/lucky/20240306/ger_py', 'users/lucky/20240306/lux_py']}}\n",
    "\n",
    "            # Example:\n",
    "            \n",
    "            for user, scraper_runs in perrys_todo.items():\n",
    "                \n",
    "                print(f\"\\n --- Destination folders for {user}: ---\")\n",
    "\n",
    "                for date_entry, search_setting_folders in scraper_runs.items():\n",
    "                    destination_folder = os.path.join('users', user, date_entry)\n",
    "                    print(f\"{destination_folder}\")\n",
    "                    \n",
    "                    for search_setting_folder in search_setting_folders:\n",
    "                        search_name = os.path.basename(search_setting_folder)\n",
    "\n",
    "                        # check if search setting folder found by glob is actually in the user config\n",
    "                        if search_name in search_settings_user:\n",
    "                            print(f\"    - {os.path.basename(search_setting_folder)} --- OK\")\n",
    "                        else:\n",
    "                            print(f\"    - {os.path.basename(search_setting_folder)} --- NOT FOUND\")\n",
    "                            return 0\n",
    "\n",
    "        return perrys_todo\n",
    "\n",
    "\n",
    "    def filter_jobs(jobs: pd.DataFrame, search_setting: dict) -> pd.DataFrame:\n",
    "            pass\n",
    "    \n",
    "\n",
    "    def send_logs() -> None:\n",
    "        pass\n",
    "\n",
    "    def send_email() -> None:\n",
    "        pass\n",
    "\n",
    "\n",
    "agent_perry = Agent_Perry()\n",
    "todo = agent_perry.perrys_todo\n",
    "print(todo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---> Config file paths found : ['users/tdawg/tdawg_config.json', 'users/lucky/lucky_config.json']\n",
      "Running user: lucky\n",
      "Running search setting: lux_py for user: lucky\n",
      "--- Path: users/lucky/20240306/lux_py/*.csv ---\n",
      "Keywords done: 14 : ['python', 'database', 'nlp', 'natural language', 'signal processing', 'prompt engineer', 'openai', 'language model', 'generative model', 'genai', 'ki', 'künstliche intelligenz', 'ai', 'artificial intelligence']\n",
      "Keywords left: 0 : []\n",
      "Running search setting: ger_py for user: lucky\n",
      "--- Path: users/lucky/20240306/ger_py/*.csv ---\n",
      "Keywords done: 14 : ['python', 'database', 'nlp', 'natural language', 'signal processing', 'prompt engineer', 'openai', 'language model', 'generative model', 'genai', 'ki', 'künstliche intelligenz', 'ai', 'artificial intelligence']\n",
      "Keywords left: 0 : []\n",
      "User: lucky - All search settings ran successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Agent_Fisher():\n",
    "    def __init__(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Agent Fisher is the muscle of the operation. It is responsible for running the scrapes.\n",
    "\n",
    "        get_user_configs - returns a list of dictionaries, each dictionary is a user config.\n",
    "        get_proxy - returns a string of the proxy to be passed to the scraper as a string.\n",
    "        get_date - returns a string of the date in the format YYYYMMDD.\n",
    "        Date, user and search_settings(stored in user_configs - eg. IT jobs in london, \n",
    "        remote teaching jobs in Germany). \n",
    "        \"\"\"\n",
    "        print(\" / ***Agent Fisher is online *** /\")\n",
    "        print(\"Instantiating Fisher...\")\n",
    "        self.date_run = self.get_date()\n",
    "        self.proxy = self.get_proxy()\n",
    "        self.user_configs = self.get_user_configs()\n",
    "\n",
    "    # --- INIT Functions --- #\n",
    "\n",
    "    @staticmethod\n",
    "    def get_date() -> str:\n",
    "        now = datetime.datetime.now()\n",
    "        date = now.strftime(\"%Y%m%d\")\n",
    "        \n",
    "        return date\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_user_configs() -> list[dict]: \n",
    "\n",
    "        user_config_paths = glob.glob('users/**/*.json')\n",
    "        print(f\" ---> Config file paths found : {user_config_paths}\")\n",
    "        user_configs = []\n",
    "\n",
    "        for user_config_path in user_config_paths:\n",
    "            with open(user_config_path) as f:\n",
    "                tmp_dict = json.load(f)\n",
    "                user_configs.append(tmp_dict)\n",
    "        \n",
    "        return user_configs\n",
    "\n",
    "    @staticmethod # proxy path is hardcoded\n",
    "    def get_proxy() -> str:\n",
    "        \n",
    "        proxy_path = os.path.join(\"results\", \"proxy.txt\")\n",
    "        with open(proxy_path, \"r\") as f:\n",
    "            proxy = f.read().strip()\n",
    "\n",
    "        return proxy\n",
    "\n",
    "\n",
    "    # --- SEARCH --- #\n",
    "\n",
    "    def update_keywords_left(self, kewords_for_this_search: list, ss_path: str) -> list[str]:\n",
    "\n",
    "        \"\"\"\n",
    "        This function checks which keywords have already been run and determines the state of the run.\n",
    "        If a keyword has already been run (clean, no exception thrown) it is removed from the list (state).\n",
    "        Exceptions are there to handle the case where the run was not completed, incomplete or the proxy\n",
    "        was blocked.\n",
    "\n",
    "        ss_path: str - user/date/search_setting - path to the search setting folder\n",
    "\n",
    "        Note - the order of the keywods run is picked randomly. To spice things up a bit ...\n",
    "        \"\"\"\n",
    "        \n",
    "        files = glob.glob(f\"{ss_path}/*.csv\") ### os path join\n",
    "        files = [os.path.basename(file) for file in files]\n",
    "\n",
    "        keywords_done = [keyword for keyword in kewords_for_this_search if f\"{keyword}.csv\" in files]\n",
    "        keywords_left = [keyword for keyword in kewords_for_this_search if keyword not in keywords_done]\n",
    "\n",
    "        print(f\"--- Path: {ss_path}/*.csv ---\")\n",
    "        print(f\"Keywords done: {len(keywords_done)} : {keywords_done}\")\n",
    "        print(f\"Keywords left: {len(keywords_left)} : {keywords_left}\")\n",
    "        return keywords_left\n",
    "\n",
    "\n",
    "    def run_search_setting(self, username: str, search_setting: dict, date: str) -> bool:\n",
    "            \n",
    "        empty_jobs = pd.DataFrame(columns=[ \"job_url\",\n",
    "        \"site\", \"title\", \"company\", \"company_url\", \"location\", \"job_type\",\n",
    "        \"date_posted\", \"interval\", \"min_amount\", \"max_amount\", \"currency\",\n",
    "        \"is_remote\", \"num_urgent_words\", \"benefits\", \"emails\", \"description\"])\n",
    "\n",
    "        keywords = search_setting['keywords']\n",
    "        jobs = empty_jobs\n",
    "\n",
    "        ss_path = os.path.join(\"users\",username, date, search_setting['name'])\n",
    "        keywords_left = self.update_keywords_left(keywords, ss_path)\n",
    "\n",
    "        # ---- Iterate over keywords ---- #\n",
    "        while keywords_left:\n",
    "\n",
    "            keyword = random.choice(keywords_left)\n",
    "            kw_path = os.path.join(ss_path, f\"{keyword}.csv\")\n",
    "\n",
    "            print(f\"Keyword: *** {keyword} *** -> Starting search...\")\n",
    "\n",
    "            try:\n",
    "                jobs = scrape_jobs(\n",
    "                    site_name=search_setting['site_name'],\n",
    "\n",
    "                    search_term=keyword,\n",
    "                    proxy=self.proxy,\n",
    "\n",
    "                    hours_old=search_setting['hours_old'],\n",
    "                    is_remote=search_setting['is_remote'],\n",
    "                    results_wanted=search_setting['results_wanted'],\n",
    "                    country_indeed=search_setting['country_indeed']  # only needed for indeed / glassdoor\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error with keyword: {keyword}\")\n",
    "                print(f\"{e}\")\n",
    "                if \"Bad proxy\" in str(e):\n",
    "                    # if the proxy is not working, the program will never advance, so we stop it.\n",
    "                    print(\"Bad proxy, stopping the program\")\n",
    "\n",
    "                    \" TODO: Outbound email to notify the admin, maybe change proxy\"\n",
    "                    return False\n",
    "                \n",
    "                elif \"Could not find any results for the search\" in str(e):\n",
    "                    # if this happens, we write an empty file and continue, because the prog does not advance\n",
    "                    # NOTE : even tho this error is thrown, searching indeed for the kw often returns results\n",
    "                    print(f\"No jobs found for keyword: {keyword}. Writing empty file...\")\n",
    "                    empty_jobs.to_csv(kw_path, quoting=csv.QUOTE_NONNUMERIC, escapechar=\"\\\\\", index=False)\n",
    "                    keywords_left = self.update_keywords_left(keywords, ss_path)\n",
    "                \n",
    "                continue\n",
    "\n",
    "                #2024-03-06 13:36:15,683 - JobSpy - ERROR - Indeed: failed to do request: Get \"https://lu.indeed.com/m/jobs?q=openai&l=luxembourg&filter=0&start=70&sort=date&fromage=3\": http: server gave HTTP response to HTTPS client\n",
    "                #HTTPSConnectionPool(host='apis.indeed.com', port=443): Max retries exceeded with url: /graphql (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1007)')))\n",
    "                #Error with keyword: openai Could not find any results for the search. /graphql (Caused by ProxyError('Unable to connect to proxy', \n",
    "\n",
    "                # Not all exceptions are critical, and for most we can try again\n",
    "                # this is why we continue and not save the empty file\n",
    "            \n",
    "            print(f\"Number of jobs found: {len(jobs)}\")\n",
    "            jobs = jobs.drop_duplicates(subset=[\"job_url\"], keep=\"first\")\n",
    "            \n",
    "            if jobs.empty:\n",
    "                print(f\"No jobs found for keyword: {keyword}. Writing empty file...\")\n",
    "                empty_jobs.to_csv(kw_path, quoting=csv.QUOTE_NONNUMERIC, escapechar=\"\\\\\", index=False)\n",
    "                \n",
    "            else:\n",
    "                print(f\"Writing jobs to file: {kw_path}\")\n",
    "                jobs.to_csv(kw_path, quoting=csv.QUOTE_NONNUMERIC, escapechar=\"\\\\\", index=False)\n",
    "                jobs = empty_jobs\n",
    "\n",
    "            keywords_left = self.update_keywords_left(keywords, ss_path)\n",
    "        # --- End of while loop --- #\n",
    "        \n",
    "        return True\n",
    "\n",
    "    # --- Overarching --- #\n",
    "\n",
    "    def run_user(self, user_config: dict) -> None:\n",
    "        \n",
    "        username = user_config['user']\n",
    "        print(f\"Running user: {username}\")\n",
    "        \n",
    "        successful_run = []\n",
    "\n",
    "        try:\n",
    "            search_settings = user_config['search_settings']\n",
    "        except KeyError:\n",
    "            print(f\"User: {username} - No search settings found - Key Error\")\n",
    "            return\n",
    "\n",
    "        if not search_settings:\n",
    "            print(f\"User: {username} - No search settings found - Empty List\")\n",
    "            return\n",
    "\n",
    "        for search_setting in user_config['search_settings']:\n",
    "\n",
    "            # try creqting the folder\n",
    "            date_path = os.path.join(\"users\", username, self.date_run)\n",
    "            search_setting_path = os.path.join(date_path, search_setting['name'])\n",
    "\n",
    "            #if folders already exist, we skip the creation. Take this into account for Perry\n",
    "            if not os.path.exists(date_path):\n",
    "                os.makedirs(date_path)\n",
    "                print(f\"Created folder: {date_path}\")\n",
    "            if not os.path.exists(search_setting_path):\n",
    "                os.makedirs(search_setting_path)\n",
    "                print(f\"Created folder: {search_setting_path}\")\n",
    "\n",
    "            print(f\"Running search setting: {search_setting['name']} for user: {username}\")\n",
    "            successful_run.append(self.run_search_setting(username, search_setting, self.date_run))\n",
    "\n",
    "        \n",
    "        if all(successful_run):\n",
    "            print(f\"User: {username} - All search settings ran successfully\")\n",
    "        else:\n",
    "            print(f\"User: {username} - Some search settings failed. Run is incomplete\")\n",
    "\n",
    "\n",
    "    def run_all_users(self):\n",
    "\n",
    "        #date = self.date_run\n",
    "        #user_configs = self.user_configs\n",
    "\n",
    "\n",
    "        #print(f\" ---> Date : {date}\")\n",
    "        pass\n",
    "\n",
    "agent_fish = Agent_Fisher()\n",
    "agent_fish.run_user(agent_fish.user_configs[1])\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-jobspy-gxDDdTXR-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
