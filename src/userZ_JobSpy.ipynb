{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, glob, os\n",
    "import random, datetime\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from jobspy import scrape_jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "ids are unique, global static\n",
    "\n",
    "RUN\n",
    " -  GET USER CONFIGS\n",
    " -  GET DATE\n",
    "\n",
    " - FOR EACH USER CONFIG\n",
    "\n",
    "    - For each search_setting\n",
    "\n",
    "        - For each KW\n",
    "\n",
    "            - Find Jobs\n",
    "\n",
    "\n",
    "Agent Fisher\n",
    "    - run tru all user, theri settings, and their keywords\n",
    "    - save the results in their corresponding folders\n",
    "\n",
    "Agent Perry\n",
    "    - ELT the data - save to the correct folder\n",
    "    - save the results in their corresponding folders\n",
    "    \n",
    "    \n",
    "    - Generate Reports and Stats\n",
    "\n",
    "\n",
    "    LOGGING\n",
    "\n",
    "    - folder structure does a lot for logging\n",
    "    - log all errors and warnings - and the files that triggered them\n",
    "    - saving files (advace state) is very permisive as of now.\n",
    "    - Empty files are not saved (they probably should be though)\n",
    "\n",
    "    -----------\n",
    "\n",
    "    - Log all errors and warnings\n",
    "    - Log all successful runs\n",
    "    - Log all failed runs\n",
    "\n",
    "     [RUN-LOGS-(package)-E-Mail]\n",
    "\n",
    "     encypted zip - stats.csv, sigma_quo.txt, \n",
    "\n",
    "     - polymorphic/other dp, encap, graph inherit\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---> Config file paths found : ['users/tdawg/tdawg_config.json', 'users/lucky/lucky_config.json']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'user': 'tdawg', 'email': 'test@test.de'},\n",
       " {'user': 'lucky',\n",
       "  'email': 's4mipojo@uni-trier.de',\n",
       "  'jobs': [{'id': 1,\n",
       "    'name': 'lux_py',\n",
       "    'site_name': ['indeed'],\n",
       "    'search_terms': ['', 'python', 'database'],\n",
       "    'results_wanted': 500,\n",
       "    'hours_old': 72,\n",
       "    'is_remote': False,\n",
       "    'country_indeed': 'luxembourg',\n",
       "    'proxy': 'proxy',\n",
       "    'run_count': 0,\n",
       "    'run_left': 0,\n",
       "    'created': '2016-01-01T00:00:00Z',\n",
       "    'updated': '2016-01-01T00:00:00Z'},\n",
       "   {'id': 2,\n",
       "    'name': 'ger_py',\n",
       "    'site_name': ['indeed'],\n",
       "    'search_terms': ['', 'python', 'database'],\n",
       "    'results_wanted': 500,\n",
       "    'hours_old': 72,\n",
       "    'is_remote': True,\n",
       "    'country_indeed': 'germany',\n",
       "    'proxy': 'proxy',\n",
       "    'status': 'running',\n",
       "    'created': '2016-01-01T00:00:00Z',\n",
       "    'updated': '2016-01-01T00:00:00Z'}]}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Agent_Perry():\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    in-file redudancy - duplicates, cross time\n",
    "    \n",
    "\n",
    "    # listing with the same title from the same company are considered duplicates, because\n",
    "        # they are usually the same job offer, just posted multiple times for different locations\n",
    "        jobs = jobs.drop_duplicates(subset=[\"title\", \"company\"], keep=\"first\")\n",
    "        print(f\"Number of (unique) jobs found: {len(jobs)}\")\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def filter_jobs(jobs: pd.DataFrame, search_setting: dict) -> pd.DataFrame:\n",
    "            pass\n",
    "    pass\n",
    "\n",
    "    pass\n",
    "\n",
    "class Agent_Krieger():\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "class Agent_Fisher():\n",
    "    def __init__(self, proxy_path: str):\n",
    "\n",
    "        \"\"\"\n",
    "        Agent Fisher is the muscle of the operation. It is responsible for running the scrapes.\n",
    "\n",
    "        get_user_configs - returns a list of dictionaries, each dictionary is a user config.\n",
    "        get_proxy - returns a string of the proxy to be passed to the scraper as a string.\n",
    "        get_date - returns a string of the date in the format YYYYMMDD.\n",
    "        Date, user and search_settings(stored in user_configs - eg. IT jobs in london, \n",
    "        remote teaching jobs in Germany). \n",
    "        \"\"\"\n",
    "        \n",
    "        self.date_run = self.get_date()\n",
    "        self.proxy = self.get_proxy()\n",
    "        self.user_configs = self.get_user_configs()\n",
    "\n",
    "    # --- INIT Functions --- #\n",
    "\n",
    "    def get_date() -> str:\n",
    "        now = datetime.datetime.now()\n",
    "        date = now.strftime(\"%Y%m%d\")\n",
    "        \n",
    "        return date\n",
    "    \n",
    "    def get_user_configs() -> list[dict]: \n",
    "\n",
    "        user_config_paths = glob.glob('users/**/*.json')\n",
    "        print(f\" ---> Config file paths found : {user_config_paths}\")\n",
    "        user_configs = []\n",
    "\n",
    "        for user_config_path in user_config_paths:\n",
    "            with open(user_config_path) as f:\n",
    "                tmp_dict = json.load(f)\n",
    "                user_configs.append(tmp_dict)\n",
    "        \n",
    "        return user_configs\n",
    "\n",
    "    def get_proxy() -> str:\n",
    "        \n",
    "        proxy_path = os.path.join(\"results\", \"proxy.txt\")\n",
    "        with open(proxy_path, \"r\") as f:\n",
    "            proxy = f.read().strip()\n",
    "\n",
    "        return proxy\n",
    "\n",
    "\n",
    "    # --- SEARCH --- #\n",
    "\n",
    "    def update_keywords_left(kewords_for_this_search: list, ss_path: str) -> list[str]:\n",
    "\n",
    "        \"\"\"\n",
    "        This function checks which keywords have already been run and determines the state of the run.\n",
    "        If a keyword has already been run (clean, no exception thrown) it is removed from the list (state).\n",
    "        Exceptions are there to handle the case where the run was not completed, incomplete or the proxy\n",
    "        was blocked.\n",
    "\n",
    "        ss_path: str - user/date/search_setting - path to the search setting folder\n",
    "\n",
    "        Note - the order of the keywods run is picked randomly. To spice things up a bit ...\n",
    "        \"\"\"\n",
    "        \n",
    "        files = glob.glob(f\"{ss_path}/*.csv\") ### os path join\n",
    "        files = [os.path.basename(file) for file in files]\n",
    "\n",
    "        keywords_done = [keyword for keyword in kewords_for_this_search if f\"{keyword}_{date}.csv\" in files]\n",
    "        keywords_left = [keyword for keyword in kewords_for_this_search if keyword not in keywords_done]\n",
    "\n",
    "        print(f\"--- Path: {f\"{ss_path}/*.csv\"} ---\")\n",
    "        print(f\"Keywords done: {len(keywords_done)} : {keywords_done}\")\n",
    "        print(f\"Keywords left: {len(keywords_left)} : {keywords_left}\")\n",
    "        return keywords_left\n",
    "\n",
    "\n",
    "\n",
    "    def run_search_setting(self, username: str, search_setting: dict, date: str) -> None:\n",
    "            \n",
    "        empty_jobs = pd.DataFrame(columns=[ \"job_url\",\n",
    "        \"site\", \"title\", \"company\", \"company_url\", \"location\", \"job_type\",\n",
    "        \"date_posted\", \"interval\", \"min_amount\", \"max_amount\", \"currency\",\n",
    "        \"is_remote\", \"num_urgent_words\", \"benefits\", \"emails\", \"description\"])\n",
    "\n",
    "        keywords = search_setting['keywords']\n",
    "        jobs = empty_jobs\n",
    "\n",
    "        ss_path = os.path.join(\"users\",username, date, search_setting['name'])\n",
    "        keywords_left = self.update_keywords_left(keywords, ss_path)\n",
    "\n",
    "        while keywords_left:\n",
    "\n",
    "            keyword = random.choice(keywords_left)\n",
    "            kw_path = os.path.join(ss_path, f\"{keyword}.csv\")\n",
    "\n",
    "            print(f\"Keyword: *** {keyword} *** -> Starting search...\")\n",
    "\n",
    "            try:\n",
    "                jobs = scrape_jobs(\n",
    "                    site_name=search_setting['site_name'],\n",
    "\n",
    "                    search_term=keyword,\n",
    "                    proxy=self.proxy,\n",
    "\n",
    "                    hours_old=search_setting['hours_old'],\n",
    "                    is_remote=search_setting['is_remote'],\n",
    "                    results_wanted=search_setting['results_wanted'],\n",
    "                    country_indeed=search_setting['country_indeed']  # only needed for indeed / glassdoor\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error with keyword: {keyword}\")\n",
    "                print(e)\n",
    "\n",
    "                if \"Bad proxy\" in str(e):\n",
    "                    # if the proxy is not working, the program will never advance, so we stop it.\n",
    "                    print(\"Bad proxy, stopping the program\")\n",
    "\n",
    "                    \" TODO: Outbound email to notify the admin, maybe change proxy\"\n",
    "                    break\n",
    "                    return false\n",
    "\n",
    "                # There is also : \"HTTPSConnectionPool(host='apis.indeed.com', port=443): Max retries \n",
    "                # exceeded with url: /graphql (Caused by ProxyError('Unable to connect to proxy', \n",
    "                # RemoteDisconnected('Remote end closed connection without response')))\" Error but\n",
    "                # but the file being written is not necessarily empty.\n",
    "\n",
    "                continue\n",
    "            \n",
    "            print(f\"Number of jobs found: {len(jobs)}\")\n",
    "            jobs = jobs.drop_duplicates(subset=[\"job_url\"], keep=\"first\")\n",
    "            \n",
    "            if jobs.empty:\n",
    "                print(f\"No jobs found for keyword: {keyword}. Writing empty file...\")\n",
    "                empty_jobs.to_csv(kw_path, quoting=csv.QUOTE_NONNUMERIC, escapechar=\"\\\\\", index=False)\n",
    "                \n",
    "            else:\n",
    "                print(f\"Writing jobs to file: {kw_path}\")\n",
    "                jobs.to_csv(kw_path, quoting=csv.QUOTE_NONNUMERIC, escapechar=\"\\\\\", index=False)\n",
    "                jobs = empty_jobs\n",
    "\n",
    "            keywords_left = self.update_keywords_left(keywords, ss_path)\n",
    "        \n",
    "        return True\n",
    "\n",
    "    # --- Overarching --- #\n",
    "\n",
    "    def run_user(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def run_all_users(self):\n",
    "\n",
    "        date = self.date_run\n",
    "        user_configs = self.user_configs\n",
    "\n",
    "        proxy_path = os.path.join(\"results\", \"proxy.txt\")\n",
    "        with open(proxy_path, \"r\") as f:\n",
    "            proxy = f.read().strip()\n",
    "\n",
    "        print(f\" ---> Date : {date}\")\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "def send_logs() -> None:\n",
    "    pass\n",
    "\n",
    "def send_email() -> None:\n",
    "    pass\n",
    "\n",
    "def run_all_users(self):\n",
    "\n",
    "    date = self.date_run()\n",
    "    user_configs = self.user_configs()\n",
    "\n",
    "    proxy_path = os.path.join(\"results\", \"proxy.txt\")\n",
    "    with open(proxy_path, \"r\") as f:\n",
    "        proxy = f.read().strip()\n",
    "\n",
    "    print(f\" ---> Date : {date}\")\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-jobspy-gxDDdTXR-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
