{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, glob, os\n",
    "import random, datetime\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from jobspy import scrape_jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "ids are unique, global static\n",
    "\n",
    "Agent Fisher\n",
    "    - run tru all users, their settings, and their keywords\n",
    "    - save the results in their corresponding folders\n",
    "\n",
    "Agent Perry\n",
    "`   \n",
    "    - add new users and jobs. check if both are valid\n",
    "\n",
    "    - ELT the data - remove duplicates, clean, etc\n",
    "    - save the (agg)results in their corresponding folders\n",
    "\n",
    "    - Generate Reports and Stats\n",
    "    - sent emails, text messages?\n",
    "\n",
    "Agent Krieger\n",
    "    - run tru all jobs and apply to them\n",
    "\n",
    "\n",
    "    LOGGING\n",
    "\n",
    "    - folder structure does a lot for logging\n",
    "    - log all errors and warnings - and the files that triggered them\n",
    "    - saving files (advace state) is ~ permisive as of now.\n",
    "\n",
    "    -----------\n",
    "\n",
    "    - Log all errors and warnings\n",
    "    - Log all successful runs\n",
    "    - Log all failed runs\n",
    "\n",
    "     [RUN-LOGS-(package)-E-Mail]\n",
    "\n",
    "     encypted zip - stats.csv, sigma_quo.txt, \n",
    "\n",
    "     - polymorphic/other dp, encap, graph inherit\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Agent_Krieger():\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " / ***Agent Perry is online *** /\n",
      "Instantiating Perry...\n",
      " ---> Config file paths found : ['users/tdawg/tdawg_config.json', 'users/lucky/lucky_config.json']\n",
      " ---> Getting Perry's To-Do List...\n",
      "User: tdawg - No search settings found - Key Error\n",
      "\n",
      " --- Destination folders for lucky: ---\n",
      "users/lucky/20240306\n",
      "    - ger_py --- OK\n",
      "    - lux_py --- OK\n",
      "users/lucky/20240307\n",
      "    - ger_py --- OK\n",
      "    - lux_py --- OK\n",
      " ---> Creating Daily Aggregates...\n",
      "\n",
      " --- Aggregating data for lucky: ---\n",
      "    - signal processing.csv --- EMPTY\n",
      "    - prompt engineer.csv --- EMPTY\n",
      "    - generative model.csv --- EMPTY\n",
      "    - openai.csv --- EMPTY\n",
      "    - genai.csv --- EMPTY\n",
      "    - prompt engineer.csv --- EMPTY\n",
      "    - generative model.csv --- EMPTY\n",
      "    - openai.csv --- EMPTY\n",
      "    - künstliche intelligenz.csv --- EMPTY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_412423/913353566.py:142: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  template = pd.concat([template, read_df])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    - ki.csv --- EMPTY\n",
      "    - signal processing.csv --- EMPTY\n",
      "    - prompt engineer.csv --- EMPTY\n",
      "    - generative model.csv --- EMPTY\n",
      "    - openai.csv --- EMPTY\n",
      "    - genai.csv --- EMPTY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_412423/913353566.py:142: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  template = pd.concat([template, read_df])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    - prompt engineer.csv --- EMPTY\n",
      "    - openai.csv --- EMPTY\n",
      "    - künstliche intelligenz.csv --- EMPTY\n",
      "    - ki.csv --- EMPTY\n",
      " ---> Aggregating the Aggregates...\n",
      "\n",
      " --- Aggregating Aggregates for lucky: ---\n",
      "67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_412423/913353566.py:173: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  agg_df = pd.concat([agg_df, read_df])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Agent_Perry():\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    Perry is the Brain of the operation. He is responsible for the ELT of the data, and the \n",
    "    generation of reports and stats.\n",
    "    \n",
    "    for each user,\n",
    "        for each date\n",
    "            one dataframe with the columns found in the kw csv (standard) as well as date and \n",
    "            config folder columns to identify the source of the data\n",
    "\n",
    "\n",
    "\n",
    "        jobs = jobs.drop_duplicates(subset=[\"title\", \"company\"], keep=\"first\")\n",
    "        print(f\"Number of (unique) jobs found: {len(jobs)}\")\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "          \n",
    "          print(\" / ***Agent Perry is online *** /\")\n",
    "          print(\"Instantiating Perry...\")\n",
    "          self.user_configs = self.get_user_configs()\n",
    "          self.perrys_todo = self.get_perrys_todo()\n",
    "    \n",
    "    # using the config files you can determine the users\n",
    "    # then determine the dates for each user\n",
    "    # then determine the search configs for each user\n",
    "\n",
    "    @staticmethod # redundant, given it is a static method chekc later\n",
    "    def get_user_configs() -> list[dict]: \n",
    "\n",
    "        user_config_paths = glob.glob('users/**/*.json')\n",
    "        print(f\" ---> Config file paths found : {user_config_paths}\")\n",
    "        user_configs = []\n",
    "\n",
    "        for user_config_path in user_config_paths:\n",
    "            with open(user_config_path) as f:\n",
    "                tmp_dict = json.load(f)\n",
    "                user_configs.append(tmp_dict)\n",
    "        \n",
    "        return user_configs\n",
    "\n",
    "\n",
    "    def get_perrys_todo(self) -> dict:\n",
    "\n",
    "        \"\"\"\n",
    "        This function helps with the ELT of the data. It returns a dictionary, which contains is \n",
    "        constructed to easily iterate over the data and apply the ELT process to it.\n",
    "\n",
    "        \"\"\"\n",
    "        print(\" ---> Getting Perry's To-Do List...\")\n",
    "        perrys_todo = {}\n",
    "\n",
    "        for user_config in self.user_configs:\n",
    "            username = user_config['user']\n",
    "\n",
    "            try:\n",
    "                #each search config folder we find has to be in this list, otherwise it can't be right.\n",
    "                search_settings_user = [search_settings['name'] for search_settings in user_config['search_settings']]\n",
    "                perrys_todo[username] = {}\n",
    "\n",
    "            except KeyError:\n",
    "                print(f\"User: {username} - No search settings found - Key Error\")\n",
    "                continue\n",
    "\n",
    "            if not search_settings_user:\n",
    "                print(f\"User: {username} - No search settings found - Empty List\")\n",
    "                continue\n",
    "\n",
    "            \n",
    "            dates = glob.glob(f'users/{username}/*')\n",
    "            dates = [os.path.basename(date) for date in dates if \".\" not in date]\n",
    "                     #(date.endswith('.json') == False) and (date.endswith('.csv') == False)]  \n",
    "\n",
    "            for date in dates:\n",
    "                search_setting_folder_paths = glob.glob(f'users/{username}/{date}/*')\n",
    "                search_setting_folder_paths = [folder for folder in search_setting_folder_paths if \".\" not in folder]\n",
    "                perrys_todo[username][date] = search_setting_folder_paths\n",
    "\n",
    "\n",
    "            # for each date in each user --->\n",
    "            #   [username][date] = destination folder\n",
    "            #   [username][date][search_setting_folderpath] = source folder(s)\n",
    "            #   ... -> {'lucky': {'20240306': ['users/lucky/20240306/ger_py', 'users/lucky/20240306/lux_py']}}\n",
    "\n",
    "            # Example:\n",
    "            \n",
    "            for user, scraper_runs in perrys_todo.items():\n",
    "                \n",
    "                print(f\"\\n --- Destination folders for {user}: ---\")\n",
    "\n",
    "                for date_entry, search_setting_folders in scraper_runs.items():\n",
    "                    destination_folder = os.path.join('users', user, date_entry)\n",
    "                    print(f\"{destination_folder}\")\n",
    "                    \n",
    "                    for search_setting_folder in search_setting_folders:\n",
    "                        search_name = os.path.basename(search_setting_folder)\n",
    "\n",
    "                        # check if search setting folder found by glob is actually in the user config\n",
    "                        if search_name in search_settings_user:\n",
    "                            print(f\"    - {os.path.basename(search_setting_folder)} --- OK\")\n",
    "                        else:\n",
    "                            print(f\"    - {os.path.basename(search_setting_folder)} --- NOT FOUND\")\n",
    "                            return 0 # *** int has no attr items Error if this happens\n",
    "\n",
    "        return perrys_todo\n",
    "\n",
    "    def daily_aggregate(self) -> None:\n",
    "\n",
    "        print(\" ---> Creating Daily Aggregates...\")\n",
    "\n",
    "        for user, scraper_runs in self.perrys_todo.items():\n",
    "\n",
    "            print(f\"\\n --- Aggregating data for {user}: ---\")\n",
    "\n",
    "            for date_entry, search_setting_folders in scraper_runs.items():\n",
    "                destination_folder = os.path.join('users', user, date_entry)\n",
    "                template = pd.DataFrame(columns=[ \"job_url\",\n",
    "                            \"site\", \"title\", \"company\", \"company_url\", \"location\", \"job_type\",\n",
    "                            \"date_posted\", \"interval\", \"min_amount\", \"max_amount\", \"currency\",\n",
    "                            \"is_remote\", \"num_urgent_words\", \"benefits\", \"emails\", \"description\",\n",
    "                            \"user\",\"date\",\"SS\",\"KW\"])\n",
    "                                \n",
    "                for search_setting_folder in search_setting_folders:\n",
    "                    files_to_concat = glob.glob(f\"{search_setting_folder}/*.csv\")\n",
    "\n",
    "                    for file in files_to_concat:\n",
    "                    \n",
    "                        read_df = pd.read_csv(file)\n",
    "\n",
    "                        if read_df.empty:\n",
    "                            print(f\"    - {os.path.basename(file)} --- EMPTY\")\n",
    "                            continue\n",
    "\n",
    "                        read_df[\"user\"] = user\n",
    "                        read_df[\"date\"] = date_entry\n",
    "                        read_df[\"SS\"] = os.path.basename(search_setting_folder)\n",
    "                        read_df[\"KW\"] = os.path.basename(file).replace(\".csv\", \"\")\n",
    "                        template = pd.concat([template, read_df])\n",
    "                \n",
    "                # listing with the same title from the same company are considered duplicates, because\n",
    "                # they are usually the same job offer, just posted multiple times for different locations\n",
    "                template = template.drop_duplicates(\"job_url\", keep=\"first\")\n",
    "                template = template.drop_duplicates(subset=[\"title\", \"company\"], keep=\"first\")\n",
    "\n",
    "                template.to_csv(f\"{destination_folder}/agg.csv\", index=False)\n",
    "\n",
    "            \n",
    "    def agg_the_agg(self) -> pd.DataFrame:\n",
    "\n",
    "        print(\" ---> Aggregating the Aggregates...\")\n",
    "\n",
    "        for user in self.perrys_todo:\n",
    "\n",
    "            agg_files = glob.glob(f\"users/{user}/**/agg.csv\")\n",
    "            if not agg_files:\n",
    "                print(f\"    - No Aggregates found for {user}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\n --- Aggregating Aggregates for {user}: ---\")\n",
    "            agg_df = pd.DataFrame(columns=[ \"job_url\",\n",
    "                            \"site\", \"title\", \"company\", \"company_url\", \"location\", \"job_type\",\n",
    "                            \"date_posted\", \"interval\", \"min_amount\", \"max_amount\", \"currency\",\n",
    "                            \"is_remote\", \"num_urgent_words\", \"benefits\", \"emails\", \"description\",\n",
    "                            \"user\",\"date\",\"SS\",\"KW\"])\n",
    "            \n",
    "            for agg_file in agg_files:\n",
    "                read_df = pd.read_csv(agg_file)\n",
    "                agg_df = pd.concat([agg_df, read_df])\n",
    "\n",
    "\n",
    "            agg_df.drop([\"interval\", \"min_amount\", \"max_amount\", \"currency\",\n",
    "                            \"num_urgent_words\", \"benefits\", \"emails\", \"description\"], axis=1, inplace=True)\n",
    "        \n",
    "\n",
    "            # sort by date, oldest first\n",
    "            agg_df = agg_df.sort_values(by=\"date\", ascending=True)\n",
    "\n",
    "            # given that the table is now sorted by date (oldest first), we can drop duplicates\n",
    "            #  --- scrapes on consecutive days will have the same job offers, so we keep the first\n",
    "            # that also mean that the last date will only contain jobs that were posted on that day,\n",
    "            # even if the scrape setting is set to 3 days!\n",
    "            agg_df = agg_df.drop_duplicates(\"job_url\", keep=\"first\")\n",
    "            agg_df = agg_df.drop_duplicates(subset=[\"title\", \"company\"], keep=\"first\")\n",
    "\n",
    "            # all jobs ever found for this user (except duplicates)\n",
    "            agg_df.to_csv(f\"users/{user}/agg_agg.csv\", index=False)\n",
    "            \n",
    "            # now we make a cut with only the final date\n",
    "\n",
    "            final_date = agg_df[\"date\"].max()\n",
    "            agg_df_quo = agg_df[agg_df[\"date\"] == final_date]\n",
    "            agg_df_quo.to_csv(f\"users/{user}/agg_agg_{final_date}.csv\", index=False)\n",
    "            \n",
    "            # save all links of the job to a txt file - to list\n",
    "            \n",
    "            list_of_links = agg_df_quo[\"job_url\"].tolist()\n",
    "\n",
    "            print(len(list_of_links))\n",
    "\n",
    "            with open(f\"users/{user}/list_of_links.txt\", \"w\") as f:\n",
    "\n",
    "                count = 0\n",
    "                for link in list_of_links:\n",
    "                    f.write(f\"{link}\\n\")\n",
    "                    count += 1\n",
    "                    if count % 25 == 0:\n",
    "                        f.write(f\"\\n     --- {count} Links ---\\n\\n\")\n",
    "                \n",
    "                if count % 25 != 0: f.write(f\"\\n     --- {count} Links ---\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    def send_logs() -> None:\n",
    "        pass\n",
    "\n",
    "    def send_email() -> None:\n",
    "        pass\n",
    "\n",
    "\n",
    "agent_perry = Agent_Perry()\n",
    "agent_perry.daily_aggregate()\n",
    "agent_perry.agg_the_agg()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " / ***Agent Fisher is online *** /\n",
      "Instantiating Fisher...\n",
      " ---> Config file paths found : ['users/tdawg/tdawg_config.json', 'users/lucky/lucky_config.json']\n",
      "Running user: lucky\n",
      "Created folder: users/lucky/20240307\n",
      "Created folder: users/lucky/20240307/lux_py\n",
      "Running search setting: lux_py for user: lucky\n",
      "--- Path: users/lucky/20240307/lux_py/*.csv ---\n",
      "Keywords done: 0 : []\n",
      "Keywords left: 14 : ['python', 'database', 'nlp', 'natural language', 'signal processing', 'prompt engineer', 'openai', 'language model', 'generative model', 'genai', 'ki', 'künstliche intelligenz', 'ai', 'artificial intelligence']\n",
      "Keyword: *** artificial intelligence *** -> Starting search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 14:53:12,136 - JobSpy - ERROR - Indeed response status code 403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of jobs found: 3\n",
      "Writing jobs to file: users/lucky/20240307/lux_py/artificial intelligence.csv\n",
      "--- Path: users/lucky/20240307/lux_py/*.csv ---\n",
      "Keywords done: 1 : ['artificial intelligence']\n",
      "Keywords left: 13 : ['python', 'database', 'nlp', 'natural language', 'signal processing', 'prompt engineer', 'openai', 'language model', 'generative model', 'genai', 'ki', 'künstliche intelligenz', 'ai']\n",
      "Keyword: *** genai *** -> Starting search...\n",
      "Number of jobs found: 1\n",
      "Writing jobs to file: users/lucky/20240307/lux_py/genai.csv\n",
      "--- Path: users/lucky/20240307/lux_py/*.csv ---\n",
      "Keywords done: 2 : ['genai', 'artificial intelligence']\n",
      "Keywords left: 12 : ['python', 'database', 'nlp', 'natural language', 'signal processing', 'prompt engineer', 'openai', 'language model', 'generative model', 'ki', 'künstliche intelligenz', 'ai']\n",
      "Keyword: *** language model *** -> Starting search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 14:53:30,583 - JobSpy - ERROR - Indeed response status code 403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of jobs found: 23\n",
      "Writing jobs to file: users/lucky/20240307/lux_py/language model.csv\n",
      "--- Path: users/lucky/20240307/lux_py/*.csv ---\n",
      "Keywords done: 3 : ['language model', 'genai', 'artificial intelligence']\n",
      "Keywords left: 11 : ['python', 'database', 'nlp', 'natural language', 'signal processing', 'prompt engineer', 'openai', 'generative model', 'ki', 'künstliche intelligenz', 'ai']\n",
      "Keyword: *** signal processing *** -> Starting search...\n",
      "Number of jobs found: 2\n",
      "Writing jobs to file: users/lucky/20240307/lux_py/signal processing.csv\n",
      "--- Path: users/lucky/20240307/lux_py/*.csv ---\n",
      "Keywords done: 4 : ['signal processing', 'language model', 'genai', 'artificial intelligence']\n",
      "Keywords left: 10 : ['python', 'database', 'nlp', 'natural language', 'prompt engineer', 'openai', 'generative model', 'ki', 'künstliche intelligenz', 'ai']\n",
      "Keyword: *** database *** -> Starting search...\n",
      "Number of jobs found: 30\n",
      "Writing jobs to file: users/lucky/20240307/lux_py/database.csv\n",
      "--- Path: users/lucky/20240307/lux_py/*.csv ---\n",
      "Keywords done: 5 : ['database', 'signal processing', 'language model', 'genai', 'artificial intelligence']\n",
      "Keywords left: 9 : ['python', 'nlp', 'natural language', 'prompt engineer', 'openai', 'generative model', 'ki', 'künstliche intelligenz', 'ai']\n",
      "Keyword: *** openai *** -> Starting search...\n",
      "Number of jobs found: 0\n",
      "No jobs found for keyword: openai. Writing empty file...\n",
      "--- Path: users/lucky/20240307/lux_py/*.csv ---\n",
      "Keywords done: 6 : ['database', 'signal processing', 'openai', 'language model', 'genai', 'artificial intelligence']\n",
      "Keywords left: 8 : ['python', 'nlp', 'natural language', 'prompt engineer', 'generative model', 'ki', 'künstliche intelligenz', 'ai']\n",
      "Keyword: *** natural language *** -> Starting search...\n",
      "Number of jobs found: 4\n",
      "Writing jobs to file: users/lucky/20240307/lux_py/natural language.csv\n",
      "--- Path: users/lucky/20240307/lux_py/*.csv ---\n",
      "Keywords done: 7 : ['database', 'natural language', 'signal processing', 'openai', 'language model', 'genai', 'artificial intelligence']\n",
      "Keywords left: 7 : ['python', 'nlp', 'prompt engineer', 'generative model', 'ki', 'künstliche intelligenz', 'ai']\n",
      "Keyword: *** nlp *** -> Starting search...\n",
      "Number of jobs found: 1\n",
      "Writing jobs to file: users/lucky/20240307/lux_py/nlp.csv\n",
      "--- Path: users/lucky/20240307/lux_py/*.csv ---\n",
      "Keywords done: 8 : ['database', 'nlp', 'natural language', 'signal processing', 'openai', 'language model', 'genai', 'artificial intelligence']\n",
      "Keywords left: 6 : ['python', 'prompt engineer', 'generative model', 'ki', 'künstliche intelligenz', 'ai']\n",
      "Keyword: *** ki *** -> Starting search...\n",
      "Number of jobs found: 0\n",
      "No jobs found for keyword: ki. Writing empty file...\n",
      "--- Path: users/lucky/20240307/lux_py/*.csv ---\n",
      "Keywords done: 9 : ['database', 'nlp', 'natural language', 'signal processing', 'openai', 'language model', 'genai', 'ki', 'artificial intelligence']\n",
      "Keywords left: 5 : ['python', 'prompt engineer', 'generative model', 'künstliche intelligenz', 'ai']\n",
      "Keyword: *** ai *** -> Starting search...\n",
      "Number of jobs found: 16\n",
      "Writing jobs to file: users/lucky/20240307/lux_py/ai.csv\n",
      "--- Path: users/lucky/20240307/lux_py/*.csv ---\n",
      "Keywords done: 10 : ['database', 'nlp', 'natural language', 'signal processing', 'openai', 'language model', 'genai', 'ki', 'ai', 'artificial intelligence']\n",
      "Keywords left: 4 : ['python', 'prompt engineer', 'generative model', 'künstliche intelligenz']\n",
      "Keyword: *** generative model *** -> Starting search...\n",
      "Number of jobs found: 1\n",
      "Writing jobs to file: users/lucky/20240307/lux_py/generative model.csv\n",
      "--- Path: users/lucky/20240307/lux_py/*.csv ---\n",
      "Keywords done: 11 : ['database', 'nlp', 'natural language', 'signal processing', 'openai', 'language model', 'generative model', 'genai', 'ki', 'ai', 'artificial intelligence']\n",
      "Keywords left: 3 : ['python', 'prompt engineer', 'künstliche intelligenz']\n",
      "Keyword: *** künstliche intelligenz *** -> Starting search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 14:55:20,202 - JobSpy - ERROR - Indeed response status code 403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of jobs found: 0\n",
      "No jobs found for keyword: künstliche intelligenz. Writing empty file...\n",
      "--- Path: users/lucky/20240307/lux_py/*.csv ---\n",
      "Keywords done: 12 : ['database', 'nlp', 'natural language', 'signal processing', 'openai', 'language model', 'generative model', 'genai', 'ki', 'künstliche intelligenz', 'ai', 'artificial intelligence']\n",
      "Keywords left: 2 : ['python', 'prompt engineer']\n",
      "Keyword: *** python *** -> Starting search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 14:55:58,293 - JobSpy - ERROR - Indeed: net/http: request canceled (Client.Timeout or context cancellation while reading body)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of jobs found: 32\n",
      "Writing jobs to file: users/lucky/20240307/lux_py/python.csv\n",
      "--- Path: users/lucky/20240307/lux_py/*.csv ---\n",
      "Keywords done: 13 : ['python', 'database', 'nlp', 'natural language', 'signal processing', 'openai', 'language model', 'generative model', 'genai', 'ki', 'künstliche intelligenz', 'ai', 'artificial intelligence']\n",
      "Keywords left: 1 : ['prompt engineer']\n",
      "Keyword: *** prompt engineer *** -> Starting search...\n",
      "Number of jobs found: 0\n",
      "No jobs found for keyword: prompt engineer. Writing empty file...\n",
      "--- Path: users/lucky/20240307/lux_py/*.csv ---\n",
      "Keywords done: 14 : ['python', 'database', 'nlp', 'natural language', 'signal processing', 'prompt engineer', 'openai', 'language model', 'generative model', 'genai', 'ki', 'künstliche intelligenz', 'ai', 'artificial intelligence']\n",
      "Keywords left: 0 : []\n",
      "Created folder: users/lucky/20240307/ger_py\n",
      "Running search setting: ger_py for user: lucky\n",
      "--- Path: users/lucky/20240307/ger_py/*.csv ---\n",
      "Keywords done: 0 : []\n",
      "Keywords left: 14 : ['python', 'database', 'nlp', 'natural language', 'signal processing', 'prompt engineer', 'openai', 'language model', 'generative model', 'genai', 'ki', 'künstliche intelligenz', 'ai', 'artificial intelligence']\n",
      "Keyword: *** nlp *** -> Starting search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucky/Documents/scripts/agent_jobspy/JobSpy/src/jobspy/__init__.py:155: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  jobs_df = pd.concat(jobs_dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of jobs found: 4\n",
      "Writing jobs to file: users/lucky/20240307/ger_py/nlp.csv\n",
      "--- Path: users/lucky/20240307/ger_py/*.csv ---\n",
      "Keywords done: 1 : ['nlp']\n",
      "Keywords left: 13 : ['python', 'database', 'natural language', 'signal processing', 'prompt engineer', 'openai', 'language model', 'generative model', 'genai', 'ki', 'künstliche intelligenz', 'ai', 'artificial intelligence']\n",
      "Keyword: *** signal processing *** -> Starting search...\n",
      "Error with keyword: signal processing\n",
      "Could not find any results for the search\n",
      "No jobs found for keyword: signal processing. Writing empty file...\n",
      "--- Path: users/lucky/20240307/ger_py/*.csv ---\n",
      "Keywords done: 2 : ['nlp', 'signal processing']\n",
      "Keywords left: 12 : ['python', 'database', 'natural language', 'prompt engineer', 'openai', 'language model', 'generative model', 'genai', 'ki', 'künstliche intelligenz', 'ai', 'artificial intelligence']\n",
      "Keyword: *** python *** -> Starting search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucky/Documents/scripts/agent_jobspy/JobSpy/src/jobspy/__init__.py:155: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  jobs_df = pd.concat(jobs_dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of jobs found: 69\n",
      "Writing jobs to file: users/lucky/20240307/ger_py/python.csv\n",
      "--- Path: users/lucky/20240307/ger_py/*.csv ---\n",
      "Keywords done: 3 : ['python', 'nlp', 'signal processing']\n",
      "Keywords left: 11 : ['database', 'natural language', 'prompt engineer', 'openai', 'language model', 'generative model', 'genai', 'ki', 'künstliche intelligenz', 'ai', 'artificial intelligence']\n",
      "Keyword: *** artificial intelligence *** -> Starting search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 14:57:12,622 - JobSpy - ERROR - Indeed response status code 403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of jobs found: 4\n",
      "Writing jobs to file: users/lucky/20240307/ger_py/artificial intelligence.csv\n",
      "--- Path: users/lucky/20240307/ger_py/*.csv ---\n",
      "Keywords done: 4 : ['python', 'nlp', 'signal processing', 'artificial intelligence']\n",
      "Keywords left: 10 : ['database', 'natural language', 'prompt engineer', 'openai', 'language model', 'generative model', 'genai', 'ki', 'künstliche intelligenz', 'ai']\n",
      "Keyword: *** künstliche intelligenz *** -> Starting search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucky/Documents/scripts/agent_jobspy/JobSpy/src/jobspy/__init__.py:155: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  jobs_df = pd.concat(jobs_dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of jobs found: 150\n",
      "Writing jobs to file: users/lucky/20240307/ger_py/künstliche intelligenz.csv\n",
      "--- Path: users/lucky/20240307/ger_py/*.csv ---\n",
      "Keywords done: 5 : ['python', 'nlp', 'signal processing', 'künstliche intelligenz', 'artificial intelligence']\n",
      "Keywords left: 9 : ['database', 'natural language', 'prompt engineer', 'openai', 'language model', 'generative model', 'genai', 'ki', 'ai']\n",
      "Keyword: *** database *** -> Starting search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 14:57:43,732 - JobSpy - ERROR - Indeed response status code 403\n",
      "/home/lucky/Documents/scripts/agent_jobspy/JobSpy/src/jobspy/__init__.py:155: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  jobs_df = pd.concat(jobs_dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of jobs found: 42\n",
      "Writing jobs to file: users/lucky/20240307/ger_py/database.csv\n",
      "--- Path: users/lucky/20240307/ger_py/*.csv ---\n",
      "Keywords done: 6 : ['python', 'database', 'nlp', 'signal processing', 'künstliche intelligenz', 'artificial intelligence']\n",
      "Keywords left: 8 : ['natural language', 'prompt engineer', 'openai', 'language model', 'generative model', 'genai', 'ki', 'ai']\n",
      "Keyword: *** generative model *** -> Starting search...\n",
      "Error with keyword: generative model\n",
      "Could not find any results for the search\n",
      "No jobs found for keyword: generative model. Writing empty file...\n",
      "--- Path: users/lucky/20240307/ger_py/*.csv ---\n",
      "Keywords done: 7 : ['python', 'database', 'nlp', 'signal processing', 'generative model', 'künstliche intelligenz', 'artificial intelligence']\n",
      "Keywords left: 7 : ['natural language', 'prompt engineer', 'openai', 'language model', 'genai', 'ki', 'ai']\n",
      "Keyword: *** natural language *** -> Starting search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucky/Documents/scripts/agent_jobspy/JobSpy/src/jobspy/__init__.py:155: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  jobs_df = pd.concat(jobs_dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of jobs found: 8\n",
      "Writing jobs to file: users/lucky/20240307/ger_py/natural language.csv\n",
      "--- Path: users/lucky/20240307/ger_py/*.csv ---\n",
      "Keywords done: 8 : ['python', 'database', 'nlp', 'natural language', 'signal processing', 'generative model', 'künstliche intelligenz', 'artificial intelligence']\n",
      "Keywords left: 6 : ['prompt engineer', 'openai', 'language model', 'genai', 'ki', 'ai']\n",
      "Keyword: *** language model *** -> Starting search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 14:58:26,796 - JobSpy - ERROR - Indeed response status code 403\n",
      "/home/lucky/Documents/scripts/agent_jobspy/JobSpy/src/jobspy/__init__.py:155: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  jobs_df = pd.concat(jobs_dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of jobs found: 39\n",
      "Writing jobs to file: users/lucky/20240307/ger_py/language model.csv\n",
      "--- Path: users/lucky/20240307/ger_py/*.csv ---\n",
      "Keywords done: 9 : ['python', 'database', 'nlp', 'natural language', 'signal processing', 'language model', 'generative model', 'künstliche intelligenz', 'artificial intelligence']\n",
      "Keywords left: 5 : ['prompt engineer', 'openai', 'genai', 'ki', 'ai']\n",
      "Keyword: *** genai *** -> Starting search...\n",
      "Error with keyword: genai\n",
      "Could not find any results for the search\n",
      "No jobs found for keyword: genai. Writing empty file...\n",
      "--- Path: users/lucky/20240307/ger_py/*.csv ---\n",
      "Keywords done: 10 : ['python', 'database', 'nlp', 'natural language', 'signal processing', 'language model', 'generative model', 'genai', 'künstliche intelligenz', 'artificial intelligence']\n",
      "Keywords left: 4 : ['prompt engineer', 'openai', 'ki', 'ai']\n",
      "Keyword: *** openai *** -> Starting search...\n",
      "Error with keyword: openai\n",
      "Could not find any results for the search\n",
      "No jobs found for keyword: openai. Writing empty file...\n",
      "--- Path: users/lucky/20240307/ger_py/*.csv ---\n",
      "Keywords done: 11 : ['python', 'database', 'nlp', 'natural language', 'signal processing', 'openai', 'language model', 'generative model', 'genai', 'künstliche intelligenz', 'artificial intelligence']\n",
      "Keywords left: 3 : ['prompt engineer', 'ki', 'ai']\n",
      "Keyword: *** ki *** -> Starting search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucky/Documents/scripts/agent_jobspy/JobSpy/src/jobspy/__init__.py:155: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  jobs_df = pd.concat(jobs_dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of jobs found: 196\n",
      "Writing jobs to file: users/lucky/20240307/ger_py/ki.csv\n",
      "--- Path: users/lucky/20240307/ger_py/*.csv ---\n",
      "Keywords done: 12 : ['python', 'database', 'nlp', 'natural language', 'signal processing', 'openai', 'language model', 'generative model', 'genai', 'ki', 'künstliche intelligenz', 'artificial intelligence']\n",
      "Keywords left: 2 : ['prompt engineer', 'ai']\n",
      "Keyword: *** ai *** -> Starting search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 14:59:41,701 - JobSpy - ERROR - Indeed: failed to do request: Get \"https://de.indeed.com/m/jobs?q=ai&l=germany&filter=0&start=130&sort=date&fromage=3&sc=0kf%3Aattr%28DSQF7%29%3B\": http: server gave HTTP response to HTTPS client\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with keyword: ai\n",
      "HTTPSConnectionPool(host='apis.indeed.com', port=443): Max retries exceeded with url: /graphql (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1007)')))\n",
      "Keyword: *** ai *** -> Starting search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucky/Documents/scripts/agent_jobspy/JobSpy/src/jobspy/__init__.py:155: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  jobs_df = pd.concat(jobs_dfs, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of jobs found: 82\n",
      "Writing jobs to file: users/lucky/20240307/ger_py/ai.csv\n",
      "--- Path: users/lucky/20240307/ger_py/*.csv ---\n",
      "Keywords done: 13 : ['python', 'database', 'nlp', 'natural language', 'signal processing', 'openai', 'language model', 'generative model', 'genai', 'ki', 'künstliche intelligenz', 'ai', 'artificial intelligence']\n",
      "Keywords left: 1 : ['prompt engineer']\n",
      "Keyword: *** prompt engineer *** -> Starting search...\n",
      "Error with keyword: prompt engineer\n",
      "Could not find any results for the search\n",
      "No jobs found for keyword: prompt engineer. Writing empty file...\n",
      "--- Path: users/lucky/20240307/ger_py/*.csv ---\n",
      "Keywords done: 14 : ['python', 'database', 'nlp', 'natural language', 'signal processing', 'prompt engineer', 'openai', 'language model', 'generative model', 'genai', 'ki', 'künstliche intelligenz', 'ai', 'artificial intelligence']\n",
      "Keywords left: 0 : []\n",
      "User: lucky - All search settings ran successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Agent_Fisher():\n",
    "    def __init__(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Agent Fisher is the muscle of the operation. It is responsible for running the scrapes.\n",
    "\n",
    "        get_user_configs - returns a list of dictionaries, each dictionary is a user config.\n",
    "        get_proxy - returns a string of the proxy to be passed to the scraper as a string.\n",
    "        get_date - returns a string of the date in the format YYYYMMDD.\n",
    "        Date, user and search_settings(stored in user_configs - eg. IT jobs in london, \n",
    "        remote teaching jobs in Germany). \n",
    "        \"\"\"\n",
    "        print(\" / ***Agent Fisher is online *** /\")\n",
    "        print(\"Instantiating Fisher...\")\n",
    "        self.date_run = self.get_date()\n",
    "        self.proxy = self.get_proxy()\n",
    "        self.user_configs = self.get_user_configs()\n",
    "\n",
    "    # --- INIT Functions --- #\n",
    "\n",
    "    @staticmethod\n",
    "    def get_date() -> str:\n",
    "        now = datetime.datetime.now()\n",
    "        date = now.strftime(\"%Y%m%d\")\n",
    "        \n",
    "        return date\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_user_configs() -> list[dict]: \n",
    "\n",
    "        user_config_paths = glob.glob('users/**/*.json')\n",
    "        print(f\" ---> Config file paths found : {user_config_paths}\")\n",
    "        user_configs = []\n",
    "\n",
    "        for user_config_path in user_config_paths:\n",
    "            with open(user_config_path) as f:\n",
    "                tmp_dict = json.load(f)\n",
    "                user_configs.append(tmp_dict)\n",
    "        \n",
    "        return user_configs\n",
    "\n",
    "    @staticmethod # proxy path is hardcoded\n",
    "    def get_proxy() -> str:\n",
    "        \n",
    "        proxy_path = os.path.join(\"results\", \"proxy.txt\")\n",
    "        with open(proxy_path, \"r\") as f:\n",
    "            proxy = f.read().strip()\n",
    "\n",
    "        return proxy\n",
    "\n",
    "\n",
    "    # --- SEARCH --- #\n",
    "\n",
    "    def update_keywords_left(self, kewords_for_this_search: list, ss_path: str) -> list[str]:\n",
    "\n",
    "        \"\"\"\n",
    "        This function checks which keywords have already been run and determines the state of the run.\n",
    "        If a keyword has already been run (clean, no exception thrown) it is removed from the list (state).\n",
    "        Exceptions are there to handle the case where the run was not completed, incomplete or the proxy\n",
    "        was blocked.\n",
    "\n",
    "        ss_path: str - user/date/search_setting - path to the search setting folder\n",
    "\n",
    "        Note - the order of the keywods run is picked randomly. To spice things up a bit ...\n",
    "        \"\"\"\n",
    "        \n",
    "        files = glob.glob(f\"{ss_path}/*.csv\") ### os path join\n",
    "        files = [os.path.basename(file) for file in files]\n",
    "\n",
    "        keywords_done = [keyword for keyword in kewords_for_this_search if f\"{keyword}.csv\" in files]\n",
    "        keywords_left = [keyword for keyword in kewords_for_this_search if keyword not in keywords_done]\n",
    "\n",
    "        print(f\"--- Path: {ss_path}/*.csv ---\")\n",
    "        print(f\"Keywords done: {len(keywords_done)} : {keywords_done}\")\n",
    "        print(f\"Keywords left: {len(keywords_left)} : {keywords_left}\")\n",
    "        return keywords_left\n",
    "\n",
    "\n",
    "    def run_search_setting(self, username: str, search_setting: dict, date: str) -> bool:\n",
    "            \n",
    "        empty_jobs = pd.DataFrame(columns=[ \"job_url\",\n",
    "        \"site\", \"title\", \"company\", \"company_url\", \"location\", \"job_type\",\n",
    "        \"date_posted\", \"interval\", \"min_amount\", \"max_amount\", \"currency\",\n",
    "        \"is_remote\", \"num_urgent_words\", \"benefits\", \"emails\", \"description\"])\n",
    "\n",
    "        keywords = search_setting['keywords']\n",
    "        jobs = empty_jobs\n",
    "\n",
    "        ss_path = os.path.join(\"users\",username, date, search_setting['name'])\n",
    "        keywords_left = self.update_keywords_left(keywords, ss_path)\n",
    "\n",
    "        # ---- Iterate over keywords ---- #\n",
    "        while keywords_left:\n",
    "\n",
    "            keyword = random.choice(keywords_left)\n",
    "            kw_path = os.path.join(ss_path, f\"{keyword}.csv\")\n",
    "\n",
    "            print(f\"Keyword: *** {keyword} *** -> Starting search...\")\n",
    "\n",
    "            try:\n",
    "                jobs = scrape_jobs(\n",
    "                    site_name=search_setting['site_name'],\n",
    "\n",
    "                    search_term=keyword,\n",
    "                    proxy=self.proxy,\n",
    "\n",
    "                    hours_old=search_setting['hours_old'],\n",
    "                    is_remote=search_setting['is_remote'],\n",
    "                    results_wanted=search_setting['results_wanted'],\n",
    "                    country_indeed=search_setting['country_indeed']  # only needed for indeed / glassdoor\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error with keyword: {keyword}\")\n",
    "                print(f\"{e}\")\n",
    "                if \"Bad proxy\" in str(e):\n",
    "                    # if the proxy is not working, the program will never advance, so we stop it.\n",
    "                    print(\"Bad proxy, stopping the program\")\n",
    "\n",
    "                    \" TODO: Outbound email to notify the admin, maybe change proxy\"\n",
    "                    return False\n",
    "                \n",
    "                elif \"Could not find any results for the search\" in str(e):\n",
    "                    # if this happens, we write an empty file and continue, because the prog does not advance\n",
    "                    # NOTE : even tho this error is thrown, searching indeed for the kw often returns results\n",
    "                    print(f\"No jobs found for keyword: {keyword}. Writing empty file...\")\n",
    "                    empty_jobs.to_csv(kw_path, quoting=csv.QUOTE_NONNUMERIC, escapechar=\"\\\\\", index=False)\n",
    "                    keywords_left = self.update_keywords_left(keywords, ss_path)\n",
    "                \n",
    "                continue\n",
    "\n",
    "                #2024-03-06 13:36:15,683 - JobSpy - ERROR - Indeed: failed to do request: Get \"https://lu.indeed.com/m/jobs?q=openai&l=luxembourg&filter=0&start=70&sort=date&fromage=3\": http: server gave HTTP response to HTTPS client\n",
    "                #HTTPSConnectionPool(host='apis.indeed.com', port=443): Max retries exceeded with url: /graphql (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1007)')))\n",
    "                #Error with keyword: openai Could not find any results for the search. /graphql (Caused by ProxyError('Unable to connect to proxy', \n",
    "\n",
    "                # Not all exceptions are critical, and for most we can try again\n",
    "                # this is why we continue and not save the empty file\n",
    "            \n",
    "            print(f\"Number of jobs found: {len(jobs)}\")\n",
    "            jobs = jobs.drop_duplicates(subset=[\"job_url\"], keep=\"first\")\n",
    "            \n",
    "            if jobs.empty:\n",
    "                print(f\"No jobs found for keyword: {keyword}. Writing empty file...\")\n",
    "                empty_jobs.to_csv(kw_path, quoting=csv.QUOTE_NONNUMERIC, escapechar=\"\\\\\", index=False)\n",
    "                \n",
    "            else:\n",
    "                print(f\"Writing jobs to file: {kw_path}\")\n",
    "                jobs.to_csv(kw_path, quoting=csv.QUOTE_NONNUMERIC, escapechar=\"\\\\\", index=False)\n",
    "                jobs = empty_jobs\n",
    "\n",
    "            keywords_left = self.update_keywords_left(keywords, ss_path)\n",
    "        # --- End of while loop --- #\n",
    "        \n",
    "        return True\n",
    "\n",
    "    # --- Overarching --- #\n",
    "\n",
    "    def run_user(self, user_config: dict) -> None:\n",
    "        \n",
    "        username = user_config['user']\n",
    "        print(f\"Running user: {username}\")\n",
    "        \n",
    "        successful_run = []\n",
    "\n",
    "        try:\n",
    "            search_settings = user_config['search_settings']\n",
    "        except KeyError:\n",
    "            print(f\"User: {username} - No search settings found - Key Error\")\n",
    "            return\n",
    "\n",
    "        if not search_settings:\n",
    "            print(f\"User: {username} - No search settings found - Empty List\")\n",
    "            return\n",
    "\n",
    "        for search_setting in user_config['search_settings']:\n",
    "\n",
    "            # try creqting the folder\n",
    "            date_path = os.path.join(\"users\", username, self.date_run)\n",
    "            search_setting_path = os.path.join(date_path, search_setting['name'])\n",
    "\n",
    "            #if folders already exist, we skip the creation. Take this into account for Perry\n",
    "            if not os.path.exists(date_path):\n",
    "                os.makedirs(date_path)\n",
    "                print(f\"Created folder: {date_path}\")\n",
    "            if not os.path.exists(search_setting_path):\n",
    "                os.makedirs(search_setting_path)\n",
    "                print(f\"Created folder: {search_setting_path}\")\n",
    "\n",
    "            print(f\"Running search setting: {search_setting['name']} for user: {username}\")\n",
    "            successful_run.append(self.run_search_setting(username, search_setting, self.date_run))\n",
    "\n",
    "        \n",
    "        if all(successful_run):\n",
    "            print(f\"User: {username} - All search settings ran successfully\")\n",
    "        else:\n",
    "            print(f\"User: {username} - Some search settings failed. Run is incomplete\")\n",
    "\n",
    "\n",
    "    def run_all_users(self):\n",
    "\n",
    "        #date = self.date_run\n",
    "        #user_configs = self.user_configs\n",
    "\n",
    "\n",
    "        #print(f\" ---> Date : {date}\")\n",
    "        pass\n",
    "\n",
    "agent_fish = Agent_Fisher()\n",
    "agent_fish.run_user(agent_fish.user_configs[1])\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-jobspy-gxDDdTXR-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
